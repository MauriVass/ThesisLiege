{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import pandapower.networks\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "np.random.seed(19)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import os\n",
    "os.sep = '/'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandapower.plotting.plotly import simple_plotly\n",
    "from pandapower.plotting.plotly import vlevel_plotly\n",
    "from pandapower.plotting.plotly import pf_res_plotly\n",
    "\n",
    "from pandapower.timeseries import DFData\n",
    "from pandapower.timeseries import OutputWriter\n",
    "from pandapower.timeseries.run_time_series import run_timeseries\n",
    "from pandapower.control import ConstControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_full = pp.networks.mv_oberrhein()\n",
    "net1, net2 = pp.networks.mv_oberrhein(separation_by_sub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High Generation')\n",
    "net, _ = pp.networks.mv_oberrhein(\"generation\",separation_by_sub=True)\n",
    "# print(net.load.loc[0])\n",
    "# print(net.sgen.loc[0])\n",
    "print(f\"Total Load {net.load['p_mw'].sum():.2f} MW, scaling factor: {net.load['scaling'].iloc[0]}\")\n",
    "print(f\"Total Generation {net.sgen['p_mw'].sum():.2f} MW, scaling factor: {net.sgen['scaling'].iloc[0]}\")\n",
    "\n",
    "print('\\nHigh Load')\n",
    "net,_ = pp.networks.mv_oberrhein(\"load\",separation_by_sub=True)\n",
    "# print(net.load.loc[0])\n",
    "# print(net.sgen.loc[0])\n",
    "print(f\"Total Load {net.load['p_mw'].sum():.2f} MW, scaling factor: {net.load['scaling'].iloc[0]}\")\n",
    "print(f\"Total Generation {net.sgen['p_mw'].sum():.2f} MW, scaling factor: {net.sgen['scaling'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot network and loadings\n",
    "from pandapower.plotting.plotly import simple_plotly\n",
    "from pandapower.plotting.plotly import vlevel_plotly\n",
    "from pandapower.plotting.plotly import pf_res_plotly\n",
    "\n",
    "_ = simple_plotly(net, bus_size=6, ext_grid_size=5)\n",
    "# _ = vlevel_plotly(net)\n",
    "# fig = pf_res_plotly(net)\n",
    "# fig.write_html(f\"images/Gyn-anm network situation{case}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced plotting\n",
    "import seaborn as sn\n",
    "import pandapower.plotting as plot\n",
    "import pandapower.topology as top\n",
    "\n",
    "mg = top.create_nxgraph(net, nogobuses=set(net.trafo.lv_bus.values) | set(net.trafo.hv_bus.values))\n",
    "colors = sn.color_palette()\n",
    "collections = []\n",
    "\n",
    "sizes = plot.get_collection_sizes(net)\n",
    "for area, color in zip(top.connected_components(mg), colors):\n",
    "\tcollections.append(plot.create_bus_collection(net, area, color=color, size=sizes[\"bus\"]/2))\n",
    "\tline_ind = net.line.loc[:, \"from_bus\"].isin(area) | net.line.loc[:, \"to_bus\"].isin(area)\n",
    "\tlines = net.line.loc[line_ind].index\n",
    "\tcollections.append(plot.create_line_collection(net, lines, color=color))\n",
    "collections.append(plot.create_ext_grid_collection(net, size=sizes[\"ext_grid\"]))\n",
    "plot.draw_collections(collections)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number buses: {len(net.bus)}')\n",
    "print(f'Number loads: {len(net.load)}')\n",
    "print(f'Number sgens: {len(net.sgen)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = {}\n",
    "l = 0 \n",
    "s = 0\n",
    "\n",
    "nothing = 'rgb(0,0,0)'\n",
    "only_load = 'rgb(255,0,0)'\n",
    "only_res = 'rgb(0,255,0)'\n",
    "both = 'rgb(0,100,255)'\n",
    "net.bus['type'] = nothing #type: 0->nothing, 1->only load, 2->only res, 3->both\n",
    "for i in bus_indices:\n",
    "\tc[i] = [0,0]\n",
    "\tif(i in list(net.load['bus']) and i in list(net.sgen['bus'])):\n",
    "\t\tnet.bus['type'].loc[i] = both\n",
    "\telif(i in list(net.load['bus'])):\n",
    "\t\tc[i][0]+=1\n",
    "\t\tl+=1\n",
    "\t\tnet.bus['type'].loc[i] = only_load\n",
    "\telif(i in list(net.sgen['bus'])):\n",
    "\t\tc[i][1]+=1\n",
    "\t\ts+=1\n",
    "\t\tnet.bus['type'].loc[i] = only_res\n",
    "\n",
    "\tif(i in list(net.load['bus'])):\n",
    "\t\tc[i][0]+=1\n",
    "\t\tl+=1\n",
    "\tif(i in list(net.sgen['bus'])):\n",
    "\t\tc[i][1]+=1\n",
    "\t\ts+=1\n",
    "print('{bus index, [load,gen]}')\n",
    "print(c)\n",
    "\n",
    "l = [[],[],[],[]]\n",
    "for k,v in c.items():\n",
    "\tif(v[0]==0 and v[1]==0): #nothing\n",
    "\t\tl[0].append(k)\n",
    "\tif(v[0]>0 and v[1]==0): #only load\n",
    "\t\tl[1].append(k)\n",
    "\tif(v[0]==0 and v[1]>0): #only sgen\n",
    "\t\tl[2].append(k)\n",
    "\tif(v[0]>0 and v[1]>0): #both\n",
    "\t\tl[3].append(k)\n",
    "print('\\nNothing: ', l[0])\n",
    "print('Only load: ', l[1])\n",
    "print('Only sgen: ', l[2])\n",
    "print('Both: ', l[3])\n",
    "print(f'Ratio: Nothing/at least 1: {( len(l[0])/( len(l[0])+len(l[1])+len(l[2])+len(l[3]) ) ):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {nothing: \"Neither\", only_load: 'Only Load', only_res: 'Only RES', both: 'Both'}\n",
    "fig = simple_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "fig.update_layout(showlegend=False)\n",
    "for i in net.bus.index:\n",
    "\tfig.add_trace(\n",
    "\t\t\tpx.scatter(\n",
    "\t\t\t\tx=[net.bus_geodata.loc[i, 'x']],\n",
    "\t\t\t\ty=[net.bus_geodata.loc[i, 'y']],\n",
    "\t\t\t\tcolor=[net.bus.loc[i,'type']]).update_traces(marker=dict(color=net.bus.loc[i,'type'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsize=6)).data[0]\n",
    "\t\t)\n",
    "fig.update_layout(showlegend=True,legend=dict(y=0.9), legend_title_text='', legend_font=dict(color='black'))\n",
    "\n",
    "elems = set()\n",
    "for i in range(len(fig.data)):\n",
    "\tif(fig.data[i].name in list(labels.keys()) and labels[fig.data[i].name] not in elems):\n",
    "\t\tfig.data[i].name = labels[fig.data[i].name]\n",
    "\t\telems.add(fig.data[i].name)\n",
    "\telse:\n",
    "\t\tfig.data[i].showlegend=False\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = []\n",
    "nc = []\n",
    "l = 0\n",
    "for i in net.load.to_numpy():\n",
    "\tif(i[1] in bus_indices):\n",
    "\t\tc.append(i[1])\n",
    "\t\tl+=1\n",
    "\telse:\n",
    "\t\tnc.append(i[1])\n",
    "print(f'Loads connected to something: {c}')\n",
    "print(f'Loads connected to nothing: {nc}')\n",
    "\n",
    "print(f'\\nLoads-> total: {len(net.load)}, connected to some bus: {l} (ratio: {(l/len(net.load)):.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = []\n",
    "nc = []\n",
    "l = 0\n",
    "for i in net.sgen.to_numpy():\n",
    "\tif(i[1] in bus_indices):\n",
    "\t\tc.append(i[1])\n",
    "\t\tl+=1\n",
    "\telse:\n",
    "\t\tnc.append(i[1])\n",
    "print(f'Sgens connected to something: {c}')\n",
    "print(f'Sgens connected to nothing: {nc}')\n",
    "\n",
    "print(f'\\nSgens-> total: {len(net.sgen)}, connected to some bus: {l} (ratio: {(l/len(net.sgen)):.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.load[net.load['scaling'] == 0].count())\n",
    "print(net.sgen[net.sgen['scaling'] == 0].count()) #All sgen scaling value set to 0 since the default scenario is 'load'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the scaling values are 0.6\n",
    "net.sgen['scaling'] = 1\n",
    "# net.load['scaling'] = 1\n",
    "#Can be changed also in the Controllers\n",
    "\n",
    "# net.sgen['sn_mva'] = 25\n",
    "# net.trafo.tap_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './TimeSeries/1-MVLV-rural-all-0-sw/'\n",
    "case = 0\n",
    "n_timesteps = 4 * 24 * 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19)\n",
    "def num_foreach_element(arr):\n",
    "\td = {}\n",
    "\tfor i in arr.to_numpy():\n",
    "\t\ti = i[0]\n",
    "\t\td[i] = d[i]+1 if i in d.keys() else 1\n",
    "\treturn d\n",
    "\t\n",
    "input_dir = './TimeSeries/1-MVLV-rural-all-0-sw/'\n",
    "n_timesteps = 4 * 24 * 365\n",
    "\n",
    "#Loads dataset\n",
    "profile_load = pd.DataFrame()\n",
    "n_load = len(net.load)\n",
    "n_res = len(net.sgen)\n",
    "# The parameter “sR” generally describes the nominal apparent power of power plants, distributed energy resources and loads\n",
    "loads = pd.read_csv(f'{input_dir}Load.csv', sep=';')\n",
    "loads_timeseries = pd.read_csv(f'{input_dir}LoadProfile.csv', sep=';')\n",
    "\n",
    "RESs = pd.read_csv(f'{input_dir}RES.csv', sep=';')\n",
    "RESs_timeseries = pd.read_csv(f'{input_dir}RESProfile.csv', sep=';')\n",
    "\n",
    "#Cases:\n",
    "# 0: low load, high generation\n",
    "# 1: normal load and generation\n",
    "# 2: high load, low generation\n",
    "#How to choose them?\n",
    "# - https://dl.acm.org/doi/pdf/10.1145/3488904.3493385 The author porposes a total power demand between 30/40 MW\n",
    "case = 1\n",
    "if(case==0):\n",
    "\tscale_factor_load = 0.9\n",
    "\tscale_factor_sgen = 1.4\n",
    "elif(case==1):\n",
    "\tscale_factor_load = 0.9\n",
    "\tscale_factor_sgen = 1.4\n",
    "elif(case==2):\n",
    "\tscale_factor_load = round(0.8/0.6)\n",
    "\tscale_factor_sgen = 0.1\n",
    "print(f'Case: {case}, scale load: {scale_factor_load}, scale sgen: {scale_factor_sgen}')\n",
    "'''\n",
    "# Papers:\n",
    "# SimBench—A Benchmark Dataset of Electric Power Systems to Compare Innovative Solutions Based on Power Flow Analysis\n",
    "# https://www.researchgate.net/profile/Christian-Spalthoff-2/publication/333903713_SimBench_Open_source_time_series_of_power_load_storage_and_generation_for_the_simulation_of_electrical_distribution_grids/links/5d889953458515cbd1b50cef/SimBench-Open-source-time-series-of-power-load-storage-and-generation-for-the-simulation-of-electrical-distribution-grids.pdf?origin=publication_detail\n",
    "# https://publica.fraunhofer.de/eprints/urn_nbn_de_0011-n-5554297.pdf\n",
    "Germany standard load profiles(SLPs): Commercial enterprises (G), households (H), agricultural holdings (L) and industrial companies\n",
    "(BL/BW) were considered as accumulated consumers, while the provided time series for electric\n",
    "vehicles (EVs) and heat pumps (HPs) were interpreted as individual consumers.\n",
    "Ending letter: A-C low consumption, M medium, H high\n",
    "#print(set(loads_timeseries.columns)) \n",
    "'''\n",
    "# net.controller = pd.DataFrame() #Reset controllers\n",
    "# index_to_select = range(n_load) #np.random.randint(0,len(loads),size=n_load)\n",
    "# load_to_add = pd.DataFrame(['G3-M','G4-M','H0-H','G0-M','G3-M','G4-M'], columns=['profile'])\n",
    "different_loads = list(set(loads.profile))\n",
    "load_to_add = different_loads * int(n_load/len(different_loads)) + [ different_loads[i] for i in np.random.randint(0,len(different_loads),size=n_load-len(different_loads) * int(n_load/len(different_loads)))]\n",
    "# index_to_select = np.random.randint(0,len(loads),size=n_load)\n",
    "# index_to_select = np.random.randint(0,len(loads),size=n_load)\n",
    "\n",
    "# loads_type = loads.loc[index_to_select,['profile']]\n",
    "loads_type = pd.DataFrame(load_to_add)\n",
    "# loads_type = pd.concat([loads_type, load_to_add])\n",
    "temp_profile_p = []\n",
    "temp_profile_q = []\n",
    "print(f'Chosen load elements by type: {num_foreach_element(loads_type)}')\n",
    "for l in loads_type.to_numpy():\n",
    "\tval = 0.25\n",
    "\tnoise = np.random.uniform(1-val,1+val,len(loads_timeseries)) #random noise in range [1-val,1+val[ -> change timeseries values by +/-val%\n",
    "\ttemp_profile_p.append( loads_timeseries[f'{l[0]}_pload'] * noise)\n",
    "\ttemp_profile_q.append( loads_timeseries[f'{l[0]}_qload'] * noise)\n",
    "\n",
    "#Loads p (in MW) \n",
    "profile_load_p = pd.concat(temp_profile_p,axis=1)[:n_timesteps]\n",
    "\n",
    "output_dir = f'{input_dir}/Results/{case}'\n",
    "profile_load_p = pd.read_csv(output_dir+\"/noscale_loads_p.csv\")\n",
    "profile_load_p = profile_load_p.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "profile_load_p.columns = net.load.index\n",
    "ds_lp = DFData(profile_load_p)\n",
    "cc_lp = ConstControl(net, 'load', 'p_mw', element_index=net.load.index, profile_name=profile_load_p.columns,\n",
    "\t\t\t\t\tdata_source=ds_lp, scale_factor=scale_factor_load)\n",
    "\n",
    "#Loads q (in MVar)\n",
    "profile_load_q = pd.concat(temp_profile_q,axis=1)[:n_timesteps]\n",
    "profile_load_q = pd.read_csv(output_dir+\"/noscale_loads_q.csv\")\n",
    "profile_load_q = profile_load_q.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "profile_load_q.columns = net.load.index\n",
    "ds_lq = DFData(profile_load_q)\n",
    "cc_lq = ConstControl(net, 'load', 'q_mvar', element_index=net.load.index, profile_name=profile_load_q.columns,\n",
    "\t\t\t\t\tdata_source=ds_lq, scale_factor=scale_factor_load)\n",
    "\n",
    "#RES p (in MW)\n",
    "RESs = RESs.drop(RESs[[ i in ['BM1', 'BM2', 'BM3', 'Hydro2'] for i in RESs.profile]].index)\n",
    "different_res = list(set(RESs.profile))\n",
    "res_to_add = different_res * int(n_res/len(different_res)) + [ different_res[i] for i in np.random.randint(0,len(different_res),size=n_res-len(different_res) * int(n_res/len(different_res)))]\n",
    "RESs_type = pd.DataFrame(res_to_add)\n",
    "# res_to_add = pd.DataFrame(['WP4','WP4','WP4','WP7','WP7','WP7'], columns=['profile'])\n",
    "# index_to_select = np.random.randint(0,len(RESs),size=n_res-len(res_to_add))\n",
    "# RESs_type = RESs.loc[index_to_select,['profile']]\n",
    "# RESs_type = pd.concat([RESs_type, res_to_add])\n",
    "temp_profile_p = []\n",
    "print(f'RES elements by type: {num_foreach_element(RESs_type)}')\n",
    "for l in RESs_type.to_numpy():\n",
    "\tval = .1\n",
    "\tnoise = np.random.uniform(1-val,1+val,len(loads_timeseries)) #random noise in range [1-val,1+val[ -> change timeseries values by +/-val%\n",
    "\ttemp_profile_p.append(RESs_timeseries[f'{l[0]}']*noise)\n",
    "\t# temp_profile_p.append(RESs_timeseries[f'{l[0]}_pload']) #Q values are not required\n",
    "\n",
    "profile_res_p = pd.concat(temp_profile_p,axis=1)[:n_timesteps]\n",
    "profile_res_p = pd.read_csv(output_dir+\"/noscale_RESs_p.csv\")\n",
    "profile_res_p = profile_res_p.drop('Unnamed: 0',axis=1)\n",
    "# profile_res_p = profile_res_p * 0.95\n",
    "profile_res_p.columns = net.sgen.index\n",
    "ds_sp = DFData(profile_res_p)\n",
    "cc_sp = ConstControl(net, 'sgen', 'p_mw', element_index=net.sgen.index, profile_name=profile_res_p.columns,\n",
    "\t\t\t\t\tdata_source=ds_sp, scale_factor=scale_factor_sgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(22, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "(cc_sp.data_source.df*cc_sp.scale_factor).sum(axis=1).plot(label='sgen')\n",
    "(cc_lp.data_source.df*cc_lp.scale_factor).sum(axis=1).plot(label='load p',ylabel='mw')\n",
    "# (cc_lq.data_source.df*cc_lq.scale_factor).sum(axis=1).plot(label='load q',ylabel='mw')\n",
    "# profile_res_p.loc[:,['PV5', 'PV8', 'PV6']].sum(axis=1).plot(label='sgen PV')\n",
    "# profile_res_p.loc[:,['WP4','WP7']].sum(axis=1).plot(label='sgen WP')\n",
    "# w = 4 * 24 * 7\n",
    "# (cc_lp.data_source.df*cc_lp.scale_factor).sum(axis=1).rolling(window = w).mean().plot(label='load p',ylabel='mw')\n",
    "# (cc_sp.data_source.df*cc_sp.scale_factor).sum(axis=1).rolling(window = w).mean().plot(label='sgen')\n",
    "\n",
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\n",
    "plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend()\n",
    "# plt.xlabel('time')\n",
    "plt.title('Total load and generation')\n",
    "plt.show()\n",
    "\n",
    "###\n",
    "#Similar energy consumption over the different months\n",
    "#https://www.eia.gov/totalenergy/data/monthly/\n",
    "#https://www.eia.gov/totalenergy/data/monthly/pdf/sec1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESs_timeseries.drop(['time','BM1', 'BM2', 'BM3', 'Hydro2'], axis=1).plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_p = [i for i in loads_timeseries.columns if i.find('p')>=0]\n",
    "for i in only_p:\n",
    "\tloads_timeseries[i].plot(figsize=(20,8))\n",
    "\tplt.title(i)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile_res_p\n",
    "df =  df.loc[:,~df.columns.duplicated()]\n",
    "df.boxplot(rot=45,figsize=figsize)\n",
    "plt.ylabel('mw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './ReinforcementLearning')\n",
    "\n",
    "from Agent import Agent\n",
    "input_dim = n_load*2+n_res\n",
    "n_actions = n_res\n",
    "rlagent = Agent(input_dim,n_actions=n_actions, fc1=128, fc2=64)\n",
    "num_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = range(0,n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = True\n",
    "train = False\n",
    "output_dir = os.path.join(input_dir,\"Results\", str(case))\n",
    "if(rl):\n",
    "\tif(train):\n",
    "\t\ttime_steps = range(0,int(n_timesteps * 0.7))\n",
    "\telse:\n",
    "\t\ttime_steps = range(int(n_timesteps * 0.7),n_timesteps)\n",
    "\t\tnum_episodes = 1\n",
    "\t\toutput_dir+='/Testing'\n",
    "else:\n",
    "\t\ttime_steps = range(0,n_timesteps)\n",
    "\n",
    "\n",
    "ow = OutputWriter(net, time_steps, output_path=output_dir, output_file_type=\".csv\", log_variables=list())\n",
    "\n",
    "#Save time series (output)\n",
    "# these variables are saved to the harddisk after / during the time series loop\n",
    "ow.log_variable('res_load', 'p_mw')\n",
    "ow.log_variable('res_load', 'q_mvar')\n",
    "ow.log_variable('res_bus', 'vm_pu')\n",
    "ow.log_variable('res_bus', 'p_mw')\n",
    "ow.log_variable('res_bus', 'q_mvar')\n",
    "# ow.log_variable('res_bus', 'va_degree')\n",
    "ow.log_variable('res_line', 'loading_percent')\n",
    "# ow.log_variable('res_line', 'i_ka')\n",
    "\n",
    "t_start = time.time()\n",
    "for i in range(num_episodes):\n",
    "\tprint('Time steps: ',len(time_steps), '. Num Loads: ', net.load.index.shape, '. Load p and q: ', profile_load_p.shape, profile_load_q.shape, '. Num RESs: ', net.sgen.index.shape, '. RESs p: ',profile_res_p.shape)\n",
    "\tt1 = time.time()\n",
    "\tif (i == num_episodes - 1):\n",
    "\t\tsave = True\n",
    "\telse:\n",
    "\t\tsave = False\n",
    "\t\n",
    "\tif(rl):\n",
    "\t\tif(train):\n",
    "\t\t\trun_timeseries(net,time_steps, rlagent, save, True)\n",
    "\t\telse:\n",
    "\t\t\trun_timeseries(net,time_steps, rlagent, True, False)\n",
    "\telse:\n",
    "\t\trun_timeseries(net,time_steps, None, True)\n",
    "\tt2 = time.time()\n",
    "\tprint(f'Simulation time : {(t2-t1):.2f} s ({((t2-t1)/60):.1f} m)\\n')\n",
    "t_end = time.time()\n",
    "print(f'Total Simulation time : {(t_end-t_start):.2f} s ({((t_end-t_start)/60):.1f} m)\\n')\n",
    "\n",
    "#Save time series (input)\n",
    "path = os.path.join(output_dir, \"loads_p.csv\")\n",
    "(cc_lp.data_source.df*cc_lp.scale_factor).to_csv(path)\n",
    "path = os.path.join(output_dir, \"loads_q.csv\")\n",
    "(cc_lq.data_source.df*cc_lq.scale_factor).to_csv(path)\n",
    "path = os.path.join(output_dir, \"RESs_p.csv\")\n",
    "(cc_sp.data_source.df*cc_sp.scale_factor).to_csv(path)\n",
    "\n",
    "path = os.path.join(output_dir, \"noscale_loads_p.csv\")\n",
    "(cc_lp.data_source.df).to_csv(path)\n",
    "path = os.path.join(output_dir, \"noscale_loads_q.csv\")\n",
    "(cc_lq.data_source.df).to_csv(path)\n",
    "path = os.path.join(output_dir, \"noscale_RESs_p.csv\")\n",
    "(cc_sp.data_source.df).to_csv(path)\n",
    "t3 = time.time()\n",
    "print(f'Saving files time: {(t3-t2):.2f} s')\n",
    "\n",
    "#Total Simulation time : 550.83 s (9.2 m)\n",
    "#Total Simulation time : 1775.19 s (29.6 m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(rlagent.history)),rlagent.history,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1000\n",
    "fig = px.line(cc_lp.data_source.df*cc_lp.scale_factor,x=time_steps, y=profile_load_p.columns, width=width, height=400)\n",
    "fig.show()\n",
    "fig = px.line(cc_lq.data_source.df*cc_lq.scale_factor,x=time_steps, y=profile_load_q.columns, width=width, height=400)\n",
    "fig.show()\n",
    "fig = px.line(cc_sp.data_source.df*cc_sp.scale_factor,x=time_steps, y=profile_res_p.columns, width=width, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './TimeSeries/1-MVLV-rural-all-0-sw/Results/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm_pu_file = f'{output_dir}/res_bus/vm_pu.csv'\n",
    "vm_pu_file = f'{output_dir}/Testing/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu_after = vm_pu.drop(columns='58' ) #External grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df,title='',x_axis='',y_axis='',file_name='', ticks=''):\n",
    "\tfig = px.line(df,x=time_steps, y=df.columns, width=1200, height=600)\n",
    "\tfig.update_layout(title=title,\n",
    "\t\t\t\t\txaxis_title=x_axis,\n",
    "\t\t\t\t\tyaxis_title=y_axis)\n",
    "\n",
    "\tx = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\tfig.update_xaxes(tickangle=-45,\n",
    "\t\t\t\t\ttickmode = 'array',\n",
    "\t\t\t\t\ttickvals = x,\n",
    "\t\t\t\t\tticktext= list(ticks))\n",
    "\tif(file_name):\n",
    "\t\tfig.write_html(file_name)\n",
    "\t\tprint(f'Saved \\'{title}\\' in \\'{file_name}\\'')\n",
    "\t# fig.show()\n",
    "\n",
    "output_dir = os.path.join(input_dir,\"Results\", str(case))\n",
    "save = False\n",
    "x_axis = ''\n",
    "\n",
    "# voltage results\n",
    "vm_pu_file = f'{output_dir}/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu = vm_pu.drop(columns='58' ) #External grid\n",
    "if(save):\n",
    "\tplot_df(vm_pu,'buses voltage magnitude',x_axis, 'bus vm [pu]', os.path.join(output_dir, \"Plots\", \"bus vm.html\"), ticks=ticks)\n",
    "\n",
    "# # line loading resulcsvts\n",
    "ll_file = os.path.join(output_dir, \"res_line\", \"loading_percent.csv\")\n",
    "line_loading = pd.read_csv(ll_file, index_col=0, sep=';')\n",
    "if(save):\n",
    "\tplot_df(line_loading,'line_loading',x_axis, 'line_loading [%]', os.path.join(output_dir, \"Plots\", \"line load.html\"), ticks=ticks)\n",
    "\n",
    "# # load results\n",
    "load_file = os.path.join(output_dir, \"res_load\", \"p_mw.csv\")\n",
    "load = pd.read_csv(load_file, index_col=0, sep=';')\n",
    "if(save):\n",
    "\tplot_df(load,'load active power',x_axis, 'p [MW]', os.path.join(output_dir, \"Plots\", \"load.html\"), ticks=ticks)\n",
    "load_file = os.path.join(output_dir, \"res_load\", \"q_mvar.csv\")\n",
    "load_q = pd.read_csv(load_file, index_col=0, sep=';')\n",
    "\n",
    "res_prepf = pd.read_csv(os.path.join(output_dir, \"RESs_p.csv\"))\n",
    "res_prepf = res_prepf.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(22, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "(res_prepf).sum(axis=1).plot(label='load p',ylabel='mw')\n",
    "(load/0.6).sum(axis=1).plot(label='sgen')\n",
    "\n",
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\n",
    "plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend()\n",
    "# plt.xlabel('time')\n",
    "plt.title('Total load and generation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "vm_pu = vm_pu_pre\n",
    "# vm_pu = vm_pu_after\n",
    "df['max'] = vm_pu.max(axis=1)\n",
    "df['min'] = vm_pu.min(axis=1)\n",
    "ch = []\n",
    "cl = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\tif(row['max']>1.05):\n",
    "\t\tch.append('r')\n",
    "\telse:\n",
    "\t\tch.append('b')\n",
    "\tif(row['min']<0.95):\n",
    "\t\tcl.append('r')\n",
    "\telse:\n",
    "\t\tcl.append('b')\n",
    "\t\t\n",
    "df['over'] = ch\n",
    "df['under'] = cl\n",
    "# # d = {True: 'Voltage problem', False: 'Normal condition'}\n",
    "# # df = df.replace(d)\n",
    "# fig = px.scatter(df,x=df.index,y='max',color='over', width=1000, height=400)\n",
    "# # fig.update_yaxes(type='category')\n",
    "# fig.update_traces(marker_size=5)\n",
    "# fig.update_layout(\n",
    "# \ttitle=\"\",\n",
    "# \txaxis_title=\"Time steps\",\n",
    "# \tyaxis_title=\"max bus vm [pu]\",\n",
    "# \tlegend_title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "s = 10\n",
    "ma = plt.scatter(x=df.index,y=df['max'],c=df['over'],s=s, marker='^')\n",
    "mi = plt.scatter(x=df.index,y=df['min'],c=df['under'],s=s, marker='v')\n",
    "\n",
    "plt.scatter(x=[],y=[],c=['r'], marker='^', s=s, label='Maxs overvoltages')\n",
    "plt.scatter(x=[],y=[],c=['b'], marker='^', s=s, label='Maxs normal')\n",
    "plt.scatter(x=[],y=[],c=['r'], marker='v', s=s, label='Mins undervoltages')\n",
    "plt.scatter(x=[],y=[],c=['b'], marker='v', s=s, label='Mins normal')\n",
    "\n",
    "# plt.axvline(x=n_timesteps * 0.65)\n",
    "# plt.axvline(x=n_timesteps * 0.85)\n",
    "\n",
    "plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot network and loadings\n",
    "\n",
    "bus = 58\n",
    "fig = simple_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "fig.add_trace(px.scatter(x=[net.bus_geodata.loc[bus, 'x']], y=[net.bus_geodata.loc[bus, 'y']],color=['r'],size=[10]).data[0])\n",
    "# _ = vlevel_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "# fig = pf_res_plotly(net, bus_size=8)\n",
    "# fig.write_html(f\"images/MVOberrhein/Half2.html\")\n",
    "# fig.write_image(f\"images/MVOberrhein/Half2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vm_pu[58]) #constant voltage?\n",
    "print(net.bus.loc[95])\n",
    "print(net.load[net.load['bus']==58]) #no load in bus 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm_pu.plot(label=\"vm_pu\", figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification sources:\n",
    "# - https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "#Naive version\n",
    "def classification_dataset(df):\n",
    "\tlabels = []\n",
    "\ta = 0\n",
    "\tacceptance_range = 0.1\n",
    "\tfor i in df.to_numpy():\n",
    "\t\tif(np.any(i>1+acceptance_range/2) or np.any(i<1-acceptance_range/2)):#or np.any(i<1-acceptance_range/2)\n",
    "\t\t\tlabels.append(1)\n",
    "\t\t\ta+=1\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(0)\n",
    "\tprint(f'Number of critical situations: {a}, over {len(df)} time steps, ratio: {(a/len(df)*100):.1f}%')\n",
    "\treturn pd.DataFrame(labels,columns=['Label'])\n",
    "\n",
    "#Less naive\n",
    "# def f(window):\n",
    "# \treturn max(window)\n",
    "# vm_pu['132'].rolling(3).apply(f)\n",
    "\n",
    "# vm_pu_pre = vm_pu_pre[int(n_timesteps*0.7):]\n",
    "vm_pu_classification = classification_dataset(vm_pu_pre)\n",
    "vm_pu_classification = classification_dataset(vm_pu_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.concat([vm_pu], axis=1)\n",
    "x = pd.concat([res_prepf], axis=1)\n",
    "y = vm_pu_classification\n",
    "deep_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_load_p = load.sum(axis=1)\n",
    "sum_load_q = load_q.sum(axis=1)\n",
    "sum_res_p = res_prepf.sum(axis=1)\n",
    "\n",
    "x = pd.concat([sum_load_p,sum_load_q,sum_res_p], axis=1)\n",
    "y = vm_pu_classification\n",
    "deep_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple transformation of the series into a new time series, which we use to remove the series dependence on time and stabilize the mean of the time series, so trend and seasonality are reduced during this transformation.\n",
    "x = x.diff(axis = 0, periods = 1)\n",
    "#Remove NAN values\n",
    "x = x.drop(index=0,axis=1)\n",
    "y = y.drop(index=0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import standardize_mapping\n",
    "# x.columns = [f'l_{name}' for name in x.columns]\n",
    "# y.columns = [f'bvm_{name}' for name in y.columns]\n",
    "df = pd.concat([x], axis=1)\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "\n",
    "if(deep_models):\n",
    "\t#Maybe 0.65, 0.85 is better\n",
    "\ttrain_portion = 0.7\n",
    "\tval_portion = 0.9\n",
    "\n",
    "\ttrain_df = df[0:int(n*train_portion)]\n",
    "\tval_df = df[int(n*train_portion):int(n*val_portion)]\n",
    "\ttest_df = df[int(n*val_portion):]\n",
    "\n",
    "\ttrain_df = pd.concat( [(train_df), y[0:int(n*train_portion)]], axis=1)\n",
    "\tval_df = pd.concat( [(val_df), y[int(n*train_portion):int(n*val_portion)]], axis=1)\n",
    "\ttest_df = pd.concat( [(test_df), y[int(n*val_portion):]], axis=1)\n",
    "\tprint(train_df.shape,val_df.shape,test_df.shape)\n",
    "else:\n",
    "\ttrain_portion = 0.8\n",
    "\ttrain_df = df[0:int(n*train_portion)]\n",
    "\ttest_df = df[int(n*train_portion):]\n",
    "\n",
    "\ttrain_mean = train_df.mean()\n",
    "\ttrain_std = train_df.std()\n",
    "\n",
    "\ttrain_df = pd.concat( [(train_df - train_mean) / train_std,y[0:int(n*train_portion)]], axis=1)\n",
    "\ttest_df = pd.concat( [(test_df - train_mean) / train_std,y[int(n*train_portion):]], axis=1)\n",
    "\tprint(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(deep_models):\n",
    "\ta = y[0:int(n*train_portion)]\n",
    "\tb = y[int(n*train_portion):int(n*val_portion)]\n",
    "\tc = y[int(n*val_portion):]\n",
    "\t# print(f'Total # of critical instants: {list(a.sum()+b.sum()+c.sum())[0]}, ratio: {(list((a.sum()+b.sum()+c.sum())/(a.count()+b.count()+c.count())*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Train set: {list(a.sum())[0]}, ratio: {(list(a.sum()/a.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Val set: {list(b.sum())[0]}, ratio: {(list(b.sum()/b.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Test set: {list(c.sum())[0]}, ratio: {(list(c.sum()/c.count()*100)[0]):.1f}%')\n",
    "else:\n",
    "\ta = y[0:int(n*train_portion)]\n",
    "\tc = y[int(n*train_portion):]\n",
    "\t# print(f'Total # of critical instants: {list(a.sum()+b.sum()+c.sum())[0]}, ratio: {(list((a.sum()+b.sum()+c.sum())/(a.count()+b.count()+c.count())*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Train set: {list(a.sum())[0]}, ratio: {(list(a.sum()/a.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Test set: {list(c.sum())[0]}, ratio: {(list(c.sum()/c.count()*100)[0]):.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(a)\n",
    "pos = a.sum()\n",
    "neg = total - pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2)\n",
    "weight_for_1 = (1 / pos) * (total / 2) / 8\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(f'Weight for class 0: {weight_for_0:.2f}')\n",
    "print(f'Weight for class 1: {weight_for_1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "\tdef __init__(self, input_width, label_width, shift,\n",
    "\t\t\t\ttrain_df=train_df, val_df=val_df, test_df=test_df,\n",
    "\t\t\t\tall_columns =None, training_columns=None, label_columns=None):\n",
    "\t\t# Store the raw data.\n",
    "\t\tself.train_df = train_df\n",
    "\t\tself.val_df = val_df\n",
    "\t\tself.test_df = test_df\n",
    "\n",
    "\t\tself.all_columns = all_columns\n",
    "\t\tself.column_indices = {name: i for i, name in enumerate(self.all_columns)}\n",
    "\t\tprint('All columns: ', self.column_indices)\n",
    "\n",
    "\t\tself.training_columns = training_columns\n",
    "\t\t# print('Train columns: ', self.column_indices_t)\n",
    "\n",
    "\t\tself.label_columns = label_columns\n",
    "\t\t# print('Label columns: ', self.column_indices_l)\n",
    "\n",
    "\t\t# Work out the window parameters.\n",
    "\t\tself.input_width = input_width\n",
    "\t\tself.label_width = label_width\n",
    "\t\tself.shift = shift\n",
    "\n",
    "\t\tself.total_window_size = input_width + shift\n",
    "\n",
    "\t\tself.input_slice = slice(0, input_width)\n",
    "\t\tself.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "\t\tself.label_start = self.total_window_size - self.label_width\n",
    "\t\tself.labels_slice = slice(self.label_start, None)\n",
    "\t\tself.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn '\\n'.join([\n",
    "\t\t\tf'Total window size: {self.total_window_size}',\n",
    "\t\t\tf'Input indices: {self.input_indices}',\n",
    "\t\t\tf'Label indices: {self.label_indices}',\n",
    "\t\t\tf'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "\tdef split_window(self, features):\n",
    "\t\tinputs = features[:, self.input_slice, :]\n",
    "\t\tindices = [self.column_indices[c] for c in self.training_columns]\n",
    "\t\t# inputs = tf.stack( [inputs[:, :, self.all_columns.index(name)] for name in self.training_columns], axis=-1)\n",
    "\t\tinputs = tf.stack( [inputs[:, :, i] for i in indices], axis=-1)\n",
    "\t\tindices = [self.column_indices[c] for c in self.training_columns]\n",
    "\n",
    "\t\t# labels = features[:, self.labels_slice, :]\n",
    "\t\tlabels = features[:, self.labels_slice, :]\n",
    "\t\tindices = [self.column_indices[c] for c in self.label_columns]\n",
    "\n",
    "\t\tlabels = tf.stack( [labels[:, :, i] for i in indices], axis=-1)\n",
    "\t\t# labels = tf.stack( l, axis=-1)\n",
    "\n",
    "\n",
    "\t\t# Slicing doesn't preserve static shape information, so set the shapes\n",
    "\t\t# manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "\t\tinputs.set_shape([None, self.input_width, None])\n",
    "\t\t# labels.set_shape([None, self.label_width, None])\n",
    "\t\tlabels = tf.reshape(labels, [-1,self.label_width])\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef plot(self, model=None):\n",
    "\t\tpredictions = []\n",
    "\t\tlabels = []\n",
    "\t\ta = 0\n",
    "\t\tfor i,l in self.test:\n",
    "\t\t\tif model is not None:\n",
    "\t\t\t\tpredictions.append(list(tf.reshape(model(i),[-1]).numpy()))\n",
    "\t\t\ta+=1\n",
    "\t\t\tlabels.append(list(tf.reshape(l,[-1]).numpy()))\n",
    "\t\tpredictions = [item for sublist in predictions for item in sublist]\n",
    "\t\tpredictions = np.round(predictions)+0.005 # for visualization purposes\n",
    "\t\tlabels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "\n",
    "\t\tx = range(len(predictions))\n",
    "\t\tplt.figure(figsize=(16,6))\n",
    "\t\ts = 1\n",
    "\t\tplt.scatter(x, predictions,s=s)\n",
    "\t\tplt.scatter(x, labels,s=s)\n",
    "\t\tplt.show()\n",
    "\t\treturn predictions,labels\n",
    "\n",
    "\tdef make_dataset(self, data):\n",
    "\t\tdata = np.array(data, dtype=np.float32)\n",
    "\t\tds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "\t\t\tdata=data,\n",
    "\t\t\ttargets=None,\n",
    "\t\t\tsequence_length=self.total_window_size,\n",
    "\t\t\tsequence_stride=1,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tbatch_size=1024)\n",
    "\t\t\t\n",
    "\t\tds = ds.map(self.split_window)\n",
    "\n",
    "\t\treturn ds\n",
    "\n",
    "\t@property\n",
    "\tdef train(self):\n",
    "\t\treturn self.make_dataset(self.train_df)\n",
    "\n",
    "\t@property\n",
    "\tdef val(self):\n",
    "\t\treturn self.make_dataset(self.val_df)\n",
    "\n",
    "\t@property\n",
    "\tdef test(self):\n",
    "\t\treturn self.make_dataset(self.test_df)\n",
    "\n",
    "\t@property\n",
    "\tdef example(self):\n",
    "\t\t\"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "\t\tresult = getattr(self, '_example', None)\n",
    "\t\tif result is None:\n",
    "\t\t\t# No example batch was found, so get one from the `.train` dataset\n",
    "\t\t\tresult = next(iter(self.test))\n",
    "\t\t# And cache it for next time\n",
    "\t\tself._example = result\n",
    "\t\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "\tdef __init__(self, input_width, label_width, mean, std, batch_size):\n",
    "\t\tself.input_width = input_width\n",
    "\t\tself.label_width = label_width\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, mean.shape[0] ])\n",
    "\t\tself.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, mean.shape[0] ])\n",
    "\n",
    "\tdef split_window(self, features):\n",
    "\t\tinputs = features[:, :-self.label_width, :-1]\n",
    "\n",
    "\t\tlabels = features[:, -self.label_width:, -1]\n",
    "\t\t\n",
    "\t\tinputs.set_shape([None, self.input_width, inputs.shape[-1]])\n",
    "\t\tlabels.set_shape([None,self.label_width])\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef normalize(self, features):\n",
    "\t\tfeatures = (features - self.mean) / (self.std + 1.e-6)\n",
    "\t\treturn features\n",
    "\n",
    "\tdef preprocess(self, features):\n",
    "\t\tinputs, labels = self.split_window(features)\n",
    "\t\tinputs = self.normalize(inputs)\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef make_dataset(self, data, train):\n",
    "\t\t# print('a',data.shape)\n",
    "\t\t#The targets is None since the labels are already inside the data\n",
    "\t\tds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "\t\t\t\t\t\tdata=data,\n",
    "\t\t\t\t\t\ttargets=None,\n",
    "\t\t\t\t\t\tsequence_length=self.input_width+self.label_width,\n",
    "\t\t\t\t\t\tsequence_stride=1,\n",
    "\t\t\t\t\t\tbatch_size=self.batch_size)\n",
    "\t\t# print(ds, ds.)\n",
    "\t\tds = ds.map(self.preprocess)\n",
    "\t\tds = ds.cache()\n",
    "\t\tif train is True:\n",
    "\t\t\tds = ds.shuffle(1024, reshuffle_each_iteration=True)\n",
    "\n",
    "\t\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 4*4\n",
    "output_window = 4\n",
    "label_columns = y.columns\n",
    "mean = train_df.mean(axis=0)[:-1] #remove labels from the standardization process\n",
    "std = train_df.std(axis=0)[:-1] #remove labels from the standardization process\n",
    "if(deep_models):\n",
    "  batch_size = 512\n",
    "else:\n",
    "  batch_size=1\n",
    "generator = WindowGenerator(input_width=input_window, label_width=output_window, mean=mean, std=std, batch_size=batch_size)\n",
    "train_ds = generator.make_dataset(train_df, True)\n",
    "if(deep_models):\n",
    "  val_ds = generator.make_dataset(val_df, False)\n",
    "test_ds = generator.make_dataset(test_df, False)\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx,yy in train_ds.take(1):\n",
    "\tprint(xx.shape) #(32,6,2)\n",
    "\tprint(yy.shape) #(32,6,1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_df.columns\n",
    "training_columns = x.columns\n",
    "label_columns = y.columns\n",
    "input_window = 4*4\n",
    "output_window = 1\n",
    "shift = output_window\n",
    "dwg = WindowGenerator(input_window,output_window,shift,all_columns=all_columns,training_columns=training_columns,label_columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in dwg.train.take(1):\n",
    "  print(f'Inputs shape (batch size, time steps, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch size, time steps, [features]): {example_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.Sequential([\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(units=32*4, activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=32*2, activation='relu'),\n",
    "\ttf.keras.layers.Dropout(0.5),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])\n",
    "cnn = tf.keras.Sequential([\n",
    "\ttf.keras.layers.Conv1D(filters=int(64*3),kernel_size=(4,),activation='relu'),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(int(64*2), activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])\n",
    "\n",
    "rnn = tf.keras.Sequential([\n",
    "\ttf.keras.layers.LSTM(units=int(64*3)),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(int(64*2), activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds is considered as 1 batch size (a,b,c) a = 1 (batch size)\n",
    "def convert_dataset_to_numpy(ds):\n",
    "\tlx = []\n",
    "\tly = []\n",
    "\tfor xx,yy in ds:\n",
    "\t\tshapes = xx.shape\n",
    "\t\txi = xx.numpy().reshape([ (shapes[1]*shapes[2]) ])\n",
    "\t\tyi = yy.numpy()\n",
    "\t\t# rand_forest.fit(yi,xi)\n",
    "\t\tlx.append(xi)\n",
    "\t\tly.append(yi)\n",
    "\tlx = np.array(lx)\n",
    "\tly = np.squeeze(np.array(ly))\n",
    "\treturn lx,ly\n",
    "\n",
    "train_timeseries = convert_dataset_to_numpy(train_ds)\n",
    "test_timeseries = convert_dataset_to_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "def get_score(classifier,train,test):\n",
    "\tclassifier.fit(train[0], train[1])\n",
    "\ty_pred = classifier.predict(test[0])\n",
    "\taccuracy = accuracy_score(test[1], y_pred)\n",
    "\tprecision = precision_score(test[1], y_pred, average='macro')\n",
    "\trecall = recall_score(test[1], y_pred, average='macro')\n",
    "\tf1_score_ = f1_score(test[1], y_pred, average='macro')\n",
    "\tprint(f\"#Accuracy: {accuracy:.5f}\")\n",
    "\tprint(f\"#Precision: {precision:.5f}\")\n",
    "\tprint(f\"#Recall: {recall:.5f}\")\n",
    "\tprint(f\"#f1 Score: {f1_score_:.5f}\")\n",
    "\treturn precision,recall,f1_score_,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=50, random_state=0)#, class_weight=class_weight\n",
    "# clf.fit(train_timeseries[0],train_timeseries[1])\n",
    "_ = get_score(clf, train_timeseries, test_timeseries)\n",
    "#Best\n",
    "# clf = sklearn.ensemble.RandomForestClassifier(n_estimators=500, max_depth=50, random_state=0)\n",
    "#Accuracy: 0.95097\n",
    "#Precision: 0.91891\n",
    "#Recall: 0.85007\n",
    "#f1 Score: 0.88019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "monitor = 'val_recall'\n",
    "def compile_and_fit(model, window, patience=6,class_weight=[], checkpoint_filepath=''):\n",
    "\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=patience,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmode='max',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tverbose=1)\n",
    "\tckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\tcheckpoint_filepath,\n",
    "\t\tmonitor='val_recall',\n",
    "\t\tverbose=0,\n",
    "\t\tsave_best_only=True,\n",
    "\t\tsave_weights_only=True,\n",
    "\t\tmode='max',\n",
    "\t\tsave_freq='epoch',\n",
    "\t\toptions=None,\n",
    "\t\tinitial_value_threshold=None,\n",
    "\t\t)\n",
    "\tprint(f\"checkpoint in {checkpoint_filepath}\")\n",
    "\n",
    "\tmodel.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "\t\t\t\toptimizer=tf.optimizers.Adam(),\n",
    "\t\t\t\tmetrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Precision(name='precision'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Recall(name='recall')]\n",
    "\t\t\t\t)\n",
    "\n",
    "\thistory = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "\t\t\t\t\t\tvalidation_data=window.val,\n",
    "\t\t\t\t\t\tcallbacks=[ckpoint],\n",
    "\t\t\t\t\t\tclass_weight=class_weight)#early_stopping\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "monitor = 'val_recall'\n",
    "def compile_and_fit(model, train, val, patience=6,class_weight=[], checkpoint_filepath=''):\n",
    "\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=patience,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmode='max',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tverbose=1)\n",
    "\tckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\tcheckpoint_filepath,\n",
    "\t\tmonitor='val_recall',\n",
    "\t\tverbose=1,\n",
    "\t\tsave_best_only=True,\n",
    "\t\tsave_weights_only=True,\n",
    "\t\tmode='max',\n",
    "\t\tsave_freq='epoch',\n",
    "\t\toptions=None,\n",
    "\t\tinitial_value_threshold=None,\n",
    "\t\t)\n",
    "\tprint(f\"checkpoint in {checkpoint_filepath}\")\n",
    "\n",
    "\tmodel.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "\t\t\t\toptimizer=tf.optimizers.Adam(),\n",
    "\t\t\t\tmetrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Precision(name='precision'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Recall(name='recall')]\n",
    "\t\t\t\t)\n",
    "\n",
    "\thistory = model.fit(train, epochs=MAX_EPOCHS,\n",
    "\t\t\t\t\t\tvalidation_data=val,\n",
    "\t\t\t\t\t\tcallbacks=[ckpoint],\n",
    "\t\t\t\t\t\tclass_weight=class_weight)#early_stopping\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense\n",
    "checkpoint_filepath = output_dir+'/dense'+'/checkpoint/'\n",
    "history = compile_and_fit(model, dwg, class_weight=[], checkpoint_filepath=checkpoint_filepath)\n",
    "# history = compile_and_fit(model, dwg, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn\n",
    "checkpoint_filepath = output_dir+'/dense'+'/checkpoint/' \n",
    "history = compile_and_fit(model, train_ds, val_ds, class_weight=[], checkpoint_filepath=checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = np.arange(0,MAX_EPOCHS)\n",
    "def plot_history(x,history,column,label_name):\n",
    "\tplt.plot(\n",
    "\t\tx,\n",
    "\t\thistory.history[column], label=label_name,\n",
    "\t\tmarker='o',\n",
    "\t\tmarkersize=3\n",
    ")\n",
    "plot_history(x_a,history,'accuracy','Accuracy')\n",
    "plot_history(x_a,history,'precision','Precision')\n",
    "plot_history(x_a,history,'recall','Recall')\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = np.arange(0,MAX_EPOCHS)\n",
    "# x = np.arange(0,42)\n",
    "plot_history(x_a,history,'val_accuracy','Val Accuracy')\n",
    "plot_history(x_a,history,'val_precision','Val Precision')\n",
    "plot_history(x_a,history,'val_recall','Val Recall')\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = dwg.plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(dwg.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(dwg.test)\n",
    "# print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx,yy in dwg.test:\n",
    "\tprint(xx.shape,len(yy),np.sum(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.evaluate(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6db62ce4cc779aef3fde0259946866918c32f10b83e429e7a4b776ccea2a2a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('gyn_anm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
