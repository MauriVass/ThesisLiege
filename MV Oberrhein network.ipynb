{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import pandapower.networks\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "np.random.seed(19)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import os\n",
    "os.sep = '/'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandapower.plotting.plotly import simple_plotly\n",
    "from pandapower.plotting.plotly import vlevel_plotly\n",
    "from pandapower.plotting.plotly import pf_res_plotly\n",
    "\n",
    "from pandapower.timeseries import DFData\n",
    "from pandapower.timeseries import OutputWriter\n",
    "from pandapower.timeseries.run_time_series import run_timeseries\n",
    "from pandapower.control import ConstControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_full = pp.networks.mv_oberrhein()\n",
    "net1, net2 = pp.networks.mv_oberrhein(separation_by_sub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.load.p_mw.sum())\n",
    "print(net.sgen.p_mw.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './ReinforcementLearning')\n",
    "\n",
    "from Agent import Agent\n",
    "input_dim = 10\n",
    "n_actions = 2*2\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/testing\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "rlagent = Agent(input_dim,n_actions=n_actions,scale_actions=1, fc1=128, fc2=64, batch_size=3,tensorboard_writer=summary_writer)#,alpha=0.001, beta=0.003, fc1=128, fc2=64\n",
    "num_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "\trlagent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.runpp(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandapower.plotting.plotly import simple_plotly\n",
    "from pandapower.plotting.plotly import vlevel_plotly\n",
    "from pandapower.plotting.plotly import pf_res_plotly\n",
    "\n",
    "# _ = simple_plotly(net)\n",
    "# _ = vlevel_plotly(net)\n",
    "vm_pu = net.res_bus.vm_pu.drop(58)\n",
    "print('#',np.max(vm_pu))\n",
    "print('#',np.min(vm_pu))\n",
    "print('#',np.mean(vm_pu))\n",
    "print('#',np.std(vm_pu))\n",
    "fig = pf_res_plotly(net)\n",
    "# 1.0509848460706623\n",
    "# 1.0\n",
    "# 1.0356294753271302\n",
    "# 0.009557855595376996\n",
    "\n",
    "# 1.0472164975218188\n",
    "# 1.0\n",
    "# 1.0320283534554817\n",
    "# 0.009233739073123632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatge_violation_function(vm_pus):\n",
    "\tviolations = []\n",
    "\ta = 0.7\n",
    "\tm = 4\n",
    "\ttollerance = 0.05\n",
    "\tfor vm in vm_pus:\n",
    "\t\tdiff = 1-vm\n",
    "\t\tq = a*(tollerance**2) - m * tollerance\n",
    "\t\tif(np.abs(diff)<tollerance):\n",
    "\t\t\tv = a*(np.abs(diff)**2)\n",
    "\t\telif(diff>0):\n",
    "\t\t\tv = m * diff + q\n",
    "\t\telse:\n",
    "\t\t\tv = -m * diff + q\n",
    "\t\tviolations.append(v)\n",
    "\treturn violations\n",
    "\n",
    "di = 0.15\n",
    "x = np.linspace(1-di/2,1+di/2,1000)\n",
    "y = np.array(volatge_violation_function(x))\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatge_violation_function([0.97,1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High Generation')\n",
    "net, _ = pp.networks.mv_oberrhein(\"generation\",separation_by_sub=True)\n",
    "# print(net.load.loc[0])\n",
    "# print(net.sgen.loc[0])\n",
    "print(f\"Total Load {net.load['p_mw'].sum():.2f} MW, scaling factor: {net.load['scaling'].iloc[0]}\")\n",
    "print(f\"Total Generation {net.sgen['p_mw'].sum():.2f} MW, scaling factor: {net.sgen['scaling'].iloc[0]}\")\n",
    "\n",
    "print('\\nHigh Load')\n",
    "net,_ = pp.networks.mv_oberrhein(\"load\",separation_by_sub=True)\n",
    "# print(net.load.loc[0])\n",
    "# print(net.sgen.loc[0])\n",
    "print(f\"Total Load {net.load['p_mw'].sum():.2f} MW, scaling factor: {net.load['scaling'].iloc[0]}\")\n",
    "print(f\"Total Generation {net.sgen['p_mw'].sum():.2f} MW, scaling factor: {net.sgen['scaling'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot network and loadings\n",
    "from pandapower.plotting.plotly import simple_plotly\n",
    "from pandapower.plotting.plotly import vlevel_plotly\n",
    "from pandapower.plotting.plotly import pf_res_plotly\n",
    "\n",
    "fig = simple_plotly(net, bus_size=6, ext_grid_size=5)\n",
    "# _ = vlevel_plotly(net)\n",
    "# fig = pf_res_plotly(net)\n",
    "# fig.write_html(f\"images/Gyn-anm network situation{case}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced plotting\n",
    "import seaborn as sn\n",
    "import pandapower.plotting as plot\n",
    "import pandapower.topology as top\n",
    "\n",
    "mg = top.create_nxgraph(net, nogobuses=set(net.trafo.lv_bus.values) | set(net.trafo.hv_bus.values))\n",
    "colors = sn.color_palette()\n",
    "collections = []\n",
    "\n",
    "sizes = plot.get_collection_sizes(net)\n",
    "for area, color in zip(top.connected_components(mg), colors):\n",
    "\tcollections.append(plot.create_bus_collection(net, area, color=color, size=sizes[\"bus\"]/2))\n",
    "\tline_ind = net.line.loc[:, \"from_bus\"].isin(area) | net.line.loc[:, \"to_bus\"].isin(area)\n",
    "\tlines = net.line.loc[line_ind].index\n",
    "\tcollections.append(plot.create_line_collection(net, lines, color=color))\n",
    "collections.append(plot.create_ext_grid_collection(net, size=sizes[\"ext_grid\"]))\n",
    "plot.draw_collections(collections)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number buses: {len(net.bus)}')\n",
    "print(f'Number loads: {len(net.load)}')\n",
    "print(f'Number sgens: {len(net.sgen)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = {}\n",
    "l = 0 \n",
    "s = 0\n",
    "\n",
    "nothing = 'rgb(0,0,0)'\n",
    "only_load = 'rgb(255,0,0)'\n",
    "only_res = 'rgb(0,255,0)'\n",
    "both = 'rgb(0,100,255)'\n",
    "net.bus['type'] = nothing #type: 0->nothing, 1->only load, 2->only res, 3->both\n",
    "for i in bus_indices:\n",
    "\tc[i] = [0,0]\n",
    "\tif(i in list(net.load['bus']) and i in list(net.sgen['bus'])):\n",
    "\t\tnet.bus['type'].loc[i] = both\n",
    "\telif(i in list(net.load['bus'])):\n",
    "\t\tc[i][0]+=1\n",
    "\t\tl+=1\n",
    "\t\tnet.bus['type'].loc[i] = only_load\n",
    "\telif(i in list(net.sgen['bus'])):\n",
    "\t\tc[i][1]+=1\n",
    "\t\ts+=1\n",
    "\t\tnet.bus['type'].loc[i] = only_res\n",
    "\n",
    "\tif(i in list(net.load['bus'])):\n",
    "\t\tc[i][0]+=1\n",
    "\t\tl+=1\n",
    "\tif(i in list(net.sgen['bus'])):\n",
    "\t\tc[i][1]+=1\n",
    "\t\ts+=1\n",
    "print('{bus index, [load,gen]}')\n",
    "print(c)\n",
    "\n",
    "l = [[],[],[],[]]\n",
    "for k,v in c.items():\n",
    "\tif(v[0]==0 and v[1]==0): #nothing\n",
    "\t\tl[0].append(k)\n",
    "\tif(v[0]>0 and v[1]==0): #only load\n",
    "\t\tl[1].append(k)\n",
    "\tif(v[0]==0 and v[1]>0): #only sgen\n",
    "\t\tl[2].append(k)\n",
    "\tif(v[0]>0 and v[1]>0): #both\n",
    "\t\tl[3].append(k)\n",
    "print('\\nNothing: ', l[0])\n",
    "print('Only load: ', l[1])\n",
    "print('Only sgen: ', l[2])\n",
    "print('Both: ', l[3])\n",
    "print(f'Ratio: Nothing/at least 1: {( len(l[0])/( len(l[0])+len(l[1])+len(l[2])+len(l[3]) ) ):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {nothing: \"Neither\", only_load: 'Only Load', only_res: 'Only RES', both: 'Both'}\n",
    "fig = simple_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "fig.update_layout(showlegend=False)\n",
    "for i in net.bus.index:\n",
    "\tfig.add_trace(\n",
    "\t\t\tpx.scatter(\n",
    "\t\t\t\tx=[net.bus_geodata.loc[i, 'x']],\n",
    "\t\t\t\ty=[net.bus_geodata.loc[i, 'y']],\n",
    "\t\t\t\tcolor=[net.bus.loc[i,'type']]).update_traces(marker=dict(color=net.bus.loc[i,'type'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsize=6)).data[0]\n",
    "\t\t)\n",
    "fig.update_layout(showlegend=True,legend=dict(y=0.9), legend_title_text='', legend_font=dict(color='black'))\n",
    "\n",
    "elems = set()\n",
    "for i in range(len(fig.data)):\n",
    "\tif(fig.data[i].name in list(labels.keys()) and labels[fig.data[i].name] not in elems):\n",
    "\t\tfig.data[i].name = labels[fig.data[i].name]\n",
    "\t\telems.add(fig.data[i].name)\n",
    "\telse:\n",
    "\t\tfig.data[i].showlegend=False\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = []\n",
    "nc = []\n",
    "l = 0\n",
    "for i in net.load.to_numpy():\n",
    "\tif(i[1] in bus_indices):\n",
    "\t\tc.append(i[1])\n",
    "\t\tl+=1\n",
    "\telse:\n",
    "\t\tnc.append(i[1])\n",
    "print(f'Loads connected to something: {c}')\n",
    "print(f'Loads connected to nothing: {nc}')\n",
    "\n",
    "print(f'\\nLoads-> total: {len(net.load)}, connected to some bus: {l} (ratio: {(l/len(net.load)):.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [int(i.split(' ')[-1]) for i in net.bus['name']]\n",
    "bus_indices = net.bus.index\n",
    "\n",
    "c = []\n",
    "nc = []\n",
    "l = 0\n",
    "for i in net.sgen.to_numpy():\n",
    "\tif(i[1] in bus_indices):\n",
    "\t\tc.append(i[1])\n",
    "\t\tl+=1\n",
    "\telse:\n",
    "\t\tnc.append(i[1])\n",
    "print(f'Sgens connected to something: {c}')\n",
    "print(f'Sgens connected to nothing: {nc}')\n",
    "\n",
    "print(f'\\nSgens-> total: {len(net.sgen)}, connected to some bus: {l} (ratio: {(l/len(net.sgen)):.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.load[net.load['scaling'] == 0].count())\n",
    "print(net.sgen[net.sgen['scaling'] == 0].count()) #All sgen scaling value set to 0 since the default scenario is 'load'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_reactive_ratio = net.load.p_mw.sum() / net.load.q_mvar.sum() #elementwise would be the same ratio\n",
    "\n",
    "#All the scaling values are 0.6\n",
    "net.sgen['scaling'] = 1\n",
    "net.load['scaling'] = 1\n",
    "\n",
    "np.random.seed(19)\n",
    "def num_foreach_element(arr):\n",
    "\td = {}\n",
    "\tfor i in arr.to_numpy():\n",
    "\t\ti = i[0]\n",
    "\t\td[i] = d[i]+1 if i in d.keys() else 1\n",
    "\treturn dict(sorted(d.items()))\n",
    "\t\n",
    "input_dir = './TimeSeries/1-MVLV-rural-all-0-sw/'\n",
    "n_timesteps = 4 * 24 * 365\n",
    "\n",
    "#Loads dataset\n",
    "profile_load = pd.DataFrame()\n",
    "n_load = len(net.load)\n",
    "n_res = len(net.sgen)\n",
    "# The parameter “sR” generally describes the nominal apparent power of power plants, distributed energy resources and loads\n",
    "loads = pd.read_csv(f'{input_dir}Load.csv', sep=';')\n",
    "loads_timeseries = pd.read_csv(f'{input_dir}LoadProfile.csv', sep=';')\n",
    "\n",
    "RESs = pd.read_csv(f'{input_dir}RES.csv', sep=';')\n",
    "RESs_timeseries = pd.read_csv(f'{input_dir}RESProfile.csv', sep=';')\n",
    "\n",
    "#Cases:\n",
    "# 0: low load, high generation\n",
    "# 1: normal load and generation\n",
    "# 2: high load, low generation\n",
    "#How to choose them?\n",
    "# - https://dl.acm.org/doi/pdf/10.1145/3488904.3493385 The author porposes a total power demand between 30/40 MW\n",
    "case = 'rl'\n",
    "if(case=='reference'):\n",
    "\tscale_factor_load = 0.8\n",
    "\tscale_factor_sgen = 1.2\n",
    "\tgenerate = True\n",
    "elif(case=='rl'):\n",
    "\tscale_factor_load = 0.8\n",
    "\tscale_factor_sgen = 1.2\n",
    "\tgenerate = False\n",
    "print(f'Case: {case}, scale load: {scale_factor_load}, scale sgen: {scale_factor_sgen}')\n",
    "'''\n",
    "# Papers:\n",
    "# SimBench—A Benchmark Dataset of Electric Power Systems to Compare Innovative Solutions Based on Power Flow Analysis\n",
    "# https://www.researchgate.net/profile/Christian-Spalthoff-2/publication/333903713_SimBench_Open_source_time_series_of_power_load_storage_and_generation_for_the_simulation_of_electrical_distribution_grids/links/5d889953458515cbd1b50cef/SimBench-Open-source-time-series-of-power-load-storage-and-generation-for-the-simulation-of-electrical-distribution-grids.pdf?origin=publication_detail\n",
    "# https://publica.fraunhofer.de/eprints/urn_nbn_de_0011-n-5554297.pdf\n",
    "Germany standard load profiles(SLPs): Commercial enterprises (G), households (H), agricultural holdings (L) and industrial companies\n",
    "(BL/BW) were considered as accumulated consumers, while the provided time series for electric\n",
    "vehicles (EVs) and heat pumps (HPs) were interpreted as individual consumers.\n",
    "Ending letter: A-C low consumption, M medium, H high\n",
    "#print(set(loads_timeseries.columns)) \n",
    "'''\n",
    "# net.controller = pd.DataFrame() #Reset controllers\n",
    "# index_to_select = range(n_load) #np.random.randint(0,len(loads),size=n_load)\n",
    "# load_to_add = pd.DataFrame(['G3-M','G4-M','H0-H','G0-M','G3-M','G4-M'], columns=['profile'])\n",
    "loads = loads.drop(loads[[ i in ['H0-G', 'H0-L'] for i in loads.profile]].index)\n",
    "different_loads = list(set(loads.profile))\n",
    "load_to_add = different_loads * int(n_load/len(different_loads)) + [ different_loads[i] for i in np.random.randint(0,len(different_loads),size=n_load-len(different_loads) * int(n_load/len(different_loads)))]\n",
    "# index_to_select = np.random.randint(0,len(loads),size=n_load)\n",
    "# index_to_select = np.random.randint(0,len(loads),size=n_load)\n",
    "\n",
    "if(generate):\n",
    "\tloads_type = pd.DataFrame(load_to_add)\n",
    "\ttemp_profile_p = []\n",
    "\ttemp_profile_q = []\n",
    "\tprint(f'Chosen load elements by type: {num_foreach_element(loads_type)}')\n",
    "\tfor l in loads_type.to_numpy():\n",
    "\t\tval = 0.15\n",
    "\t\tnoise = np.random.uniform(1-val,1+val,len(loads_timeseries)) #random noise in range [1-val,1+val[ -> change timeseries values by +/-val%\n",
    "\t\ttemp_profile_p.append( loads_timeseries[f'{l[0]}_pload'] * noise)\n",
    "\t\ttemp_profile_q.append( loads_timeseries[f'{l[0]}_qload'] * noise)\n",
    "\n",
    "\t#Loads p (in MW) \n",
    "\tprofile_load_p = pd.concat(temp_profile_p,axis=1)[:n_timesteps]\n",
    "\t#Loads q (in MVar)\n",
    "\t# profile_load_q = pd.concat(temp_profile_q,axis=1)[:n_timesteps]\n",
    "\tprofile_load_q = profile_load_p.copy() / active_reactive_ratio\n",
    "else:\n",
    "\tprofile_load_p = pd.read_csv(input_dir+\"/Results/reference/noscale_loads_p.csv\")\n",
    "\tprofile_load_p = profile_load_p.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\tprofile_load_q = pd.read_csv(input_dir+\"/Results/reference/noscale_loads_q.csv\")\n",
    "\tprofile_load_q = profile_load_q.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "profile_load_p.columns = net.load.index\n",
    "ds_lp = DFData(profile_load_p)\n",
    "cc_lp = ConstControl(net, 'load', 'p_mw', element_index=net.load.index, profile_name=profile_load_p.columns,\n",
    "\t\t\t\t\tdata_source=ds_lp, scale_factor=scale_factor_load)\n",
    "\n",
    "profile_load_q.columns = net.load.index\n",
    "ds_lq = DFData(profile_load_q)\n",
    "cc_lq = ConstControl(net, 'load', 'q_mvar', element_index=net.load.index, profile_name=profile_load_q.columns,\n",
    "\t\t\t\t\tdata_source=ds_lq, scale_factor=scale_factor_load)\n",
    "\n",
    "#RES p (in MW)\n",
    "RESs = RESs.drop(RESs[[ i in ['BM1', 'BM2', 'BM3', 'Hydro2', 'PV8'] for i in RESs.profile]].index)\n",
    "different_res = list(set(RESs.profile))\n",
    "res_to_add = different_res * int(n_res/len(different_res)) + [ different_res[i] for i in np.random.randint(0,len(different_res),size=n_res-len(different_res) * int(n_res/len(different_res)))]\n",
    "RESs_type = pd.DataFrame(res_to_add)\n",
    "# res_to_add = pd.DataFrame(['WP4','WP4','WP4','WP7','WP7','WP7'], columns=['profile'])\n",
    "# index_to_select = np.random.randint(0,len(RESs),size=n_res-len(res_to_add))\n",
    "# RESs_type = RESs.loc[index_to_select,['profile']]\n",
    "# RESs_type = pd.concat([RESs_type, res_to_add])\n",
    "if(generate):\n",
    "\ttemp_profile_p = []\n",
    "\tprint(f'RES elements by type: {num_foreach_element(RESs_type)}')\n",
    "\tfor l in RESs_type.to_numpy():\n",
    "\t\tval = .15\n",
    "\t\tnoise = np.random.uniform(1-val,1+val,len(loads_timeseries)) #random noise in range [1-val,1+val[ -> change timeseries values by +/-val%\n",
    "\t\ttemp_profile_p.append(RESs_timeseries[f'{l[0]}']*noise)\n",
    "\t\t# temp_profile_p.append(RESs_timeseries[f'{l[0]}_pload']) #Q values are not required\n",
    "\tprofile_res_p = pd.concat(temp_profile_p,axis=1)[:n_timesteps]\n",
    "else:\n",
    "\tprofile_res_p = pd.read_csv(input_dir+\"/Results/reference/noscale_RESs_p.csv\")\n",
    "\tprofile_res_p = profile_res_p.drop('Unnamed: 0',axis=1)\n",
    "profile_res_p.columns = net.sgen.index\n",
    "net.sgen.sn_mva = np.max(profile_res_p,axis=0)\n",
    "ds_sp = DFData(profile_res_p)\n",
    "cc_sp = ConstControl(net, 'sgen', 'p_mw', element_index=net.sgen.index, profile_name=profile_res_p.columns,\n",
    "\t\t\t\t\tdata_source=ds_sp, scale_factor=scale_factor_sgen)\n",
    "\n",
    "# profile_res_p = pd.read_csv(input_dir+\"/Results/reference/noscale_RESs_p.csv\")\n",
    "# profile_res_p = profile_res_p.drop('Unnamed: 0',axis=1)\n",
    "profile_res_q = profile_res_p.copy()\n",
    "profile_res_q[:] = 0\n",
    "# profile_res_q[:] = np.sqrt(np.max(profile_res_p,axis=0)**2 - profile_res_q**2)\n",
    "profile_res_q.columns = net.sgen.index\n",
    "ds_sq = DFData(profile_res_q)\n",
    "cc_sq = ConstControl(net, 'sgen', 'q_mvar', element_index=net.sgen.index, profile_name=profile_res_q.columns,\n",
    "\t\t\t\t\tdata_source=ds_sq, scale_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(22, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "(cc_sp.data_source.df*cc_sp.scale_factor).sum(axis=1).plot(label='sgen')\n",
    "(cc_lp.data_source.df*cc_lp.scale_factor).sum(axis=1).plot(label='load p',ylabel='mw')\n",
    "(cc_lq.data_source.df*cc_lq.scale_factor).sum(axis=1).plot(label='load q',ylabel='mw')\n",
    "(cc_sq.data_source.df).sum(axis=1).plot(label='sgen q',ylabel='mw')\n",
    "# profile_res_p.loc[:,['PV5', 'PV8', 'PV6']].sum(axis=1).plot(label='sgen PV')\n",
    "# profile_res_p.loc[:,['WP4','WP7']].sum(axis=1).plot(label='sgen WP')\n",
    "# w = 4 * 24 * 7\n",
    "# (cc_lp.data_source.df*cc_lp.scale_factor).sum(axis=1).rolling(window = w).mean().plot(label='load p',ylabel='mw')\n",
    "# (cc_sp.data_source.df*cc_sp.scale_factor).sum(axis=1).rolling(window = w).mean().plot(label='sgen')\n",
    "\n",
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\n",
    "plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend()\n",
    "# plt.xlabel('time')\n",
    "plt.title('Total load and generation')\n",
    "plt.show()\n",
    "\n",
    "###\n",
    "#Similar energy consumption over the different months\n",
    "#https://www.eia.gov/totalenergy/data/monthly/\n",
    "#https://www.eia.gov/totalenergy/data/monthly/pdf/sec1.pdf\n",
    "print(np.max((cc_sp.data_source.df*cc_sp.scale_factor).sum(axis=1)))\n",
    "print(np.max((cc_lp.data_source.df*cc_lp.scale_factor).sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(profile_res_p.columns)):\n",
    "\tplt.plot(profile_res_p.iloc[:,i], label=i)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESs_timeseries.drop(['time','BM1', 'BM2', 'BM3', 'Hydro2'], axis=1).plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_p = [i for i in loads_timeseries.columns if i.find('p')>=0]\n",
    "for i in only_p:\n",
    "\tloads_timeseries[i].plot(figsize=(20,8))\n",
    "\tplt.title(i)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = RESs_timeseries\n",
    "df =  df.loc[:,~df.columns.duplicated()]\n",
    "df.boxplot(rot=45,figsize=(20,8))\n",
    "plt.ylabel('mw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_p = [i for i in loads_timeseries.columns if i.find('p')>=0]\n",
    "for i in only_p:\n",
    "\tloads_timeseries[i].plot(figsize=(20,8))\n",
    "\tplt.title(i)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# !rm -r logs\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './ReinforcementLearning')\n",
    "\n",
    "from Agent import Agent\n",
    "input_dim = n_load*2+n_res\n",
    "n_actions = 2*n_res\n",
    "rlagent = Agent(input_dim,n_actions=n_actions,scale_actions=1, fc1=256, fc2=128,\n",
    "\tbatch_size=64,alpha=0.0001, beta=0.001, tau=0.001, tensorboard_writer=summary_writer)#,alpha=0.001, beta=0.003, fc1=128, fc2=64\n",
    "num_episodes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_pu_file = f'./TimeSeries/1-MVLV-rural-all-0-sw/Results/reference/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu_before = vm_pu.drop(columns='58' ) #External grid\n",
    "#Classification sources:\n",
    "# - https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "#Naive version\n",
    "def classification_dataset(df):\n",
    "\tlabels = []\n",
    "\tln = []\n",
    "\ta = 0\n",
    "\tn=0\n",
    "\tacceptance_range = 0.1\n",
    "\tfor i in df.to_numpy():\n",
    "\t\tif(np.any(i>1+acceptance_range/2) or np.any(i<1-acceptance_range/2)):# \n",
    "\t\t\tlabels.append(1)\n",
    "\t\t\ta+=1\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\tif(np.any(i<1-acceptance_range/2)):#\n",
    "\t\t\tln.append(1)\n",
    "\t\t\tn+=1\n",
    "\t\telse:\n",
    "\t\t\tln.append(0)\n",
    "\tprint(f'Number of critical situations: {a}, over {len(df)} time steps, ratio: {(a/len(df)*100):.1f}%')\n",
    "\tprint(f'Number of critical situations(underv): {n}, over {len(ln)} time steps, ratio: {(n/len(ln)*100):.1f}%')\n",
    "\treturn pd.DataFrame(labels,columns=['Label'])\n",
    "\n",
    "#Less naive\n",
    "# def f(window):\n",
    "# \treturn max(window)\n",
    "# vm_pu['132'].rolling(3).apply(f)\n",
    "\n",
    "vm_pu_classification = classification_dataset(vm_pu)\n",
    "\n",
    "# Number of critical situations: 957, over 21024 time steps, ratio: 4.6%\n",
    "# Number of critical situations(underv): 138, over 21024 time steps, ratio: 0.7%\n",
    "# Number of critical situations: 828, over 21024 time steps, ratio: 3.9%\n",
    "# Number of critical situations(underv): 143, over 21024 time steps, ratio: 0.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = True\n",
    "train = False #RUN CELL ABOVE\n",
    "output_dir = os.path.join(input_dir,\"Results\", str(case))\n",
    "if(rl):\n",
    "\tsplit = 0.4\n",
    "\tif(train):\n",
    "\t\ttime_steps = range(0,int(n_timesteps * split))\n",
    "\t\tnum_episodes = 3\n",
    "\t\tprint(f'Training on {len(time_steps)} elements for {num_episodes} episodes')\n",
    "\n",
    "\t\tif(True):\n",
    "\t\t\tmean_load_p = (cc_lp.data_source.df[:len(time_steps)]*cc_lp.scale_factor).mean(axis=0)\n",
    "\t\t\tstd_load_p = (cc_lp.data_source.df[:len(time_steps)]*cc_lp.scale_factor).std(axis=0)\n",
    "\t\t\tmean_load_q = (cc_lq.data_source.df[:len(time_steps)]*cc_lq.scale_factor).mean(axis=0)\n",
    "\t\t\tstd_load_q = (cc_lq.data_source.df[:len(time_steps)]*cc_lq.scale_factor).std(axis=0)\n",
    "\t\t\tmean_sgen_p = (cc_sp.data_source.df[:len(time_steps)]*cc_sp.scale_factor).mean(axis=0)\n",
    "\t\t\tstd_sgen_p = (cc_sp.data_source.df[:len(time_steps)]*cc_sp.scale_factor).std(axis=0)\n",
    "\t\t\tmeans = np.transpose(np.concatenate([mean_load_p,mean_load_q,mean_sgen_p], axis=0))\n",
    "\t\t\tstds = np.transpose(np.concatenate([std_load_p,std_load_q,std_sgen_p], axis=0))\n",
    "\t\t\trlagent.set_means_stds(means,stds)\n",
    "\telse:\n",
    "\t\ttime_steps = range(int(n_timesteps * split),n_timesteps)\n",
    "\t\tnum_episodes = 1\n",
    "\t\toutput_dir+='/Testing'\n",
    "\t\tprint(f'Testing on {len(time_steps)} elements')\n",
    "else:\n",
    "\t\ttime_steps = range(0,n_timesteps)\n",
    "\t\tnum_episodes = 1\n",
    "\n",
    "\n",
    "ow = OutputWriter(net, time_steps, output_path=output_dir, output_file_type=\".csv\", log_variables=list())\n",
    "\n",
    "#Save time series (output)\n",
    "# these variables are saved to the harddisk after / during the time series loop\n",
    "ow.log_variable('res_load', 'p_mw')\n",
    "ow.log_variable('res_load', 'q_mvar')\n",
    "ow.log_variable('res_bus', 'vm_pu')\n",
    "ow.log_variable('res_bus', 'p_mw')\n",
    "ow.log_variable('res_bus', 'q_mvar')\n",
    "# ow.log_variable('res_bus', 'va_degree')\n",
    "ow.log_variable('res_line', 'loading_percent')\n",
    "# ow.log_variable('res_line', 'i_ka')\n",
    "ow.log_variable('res_ext_grid', 'p_mw')\n",
    "ow.log_variable('res_ext_grid', 'q_mvar')\n",
    "\n",
    "t_start = time.time()\n",
    "print('Time steps: ',len(time_steps), '. Num Loads: ', net.load.index.shape, '. Load p and q: ', profile_load_p.shape, profile_load_q.shape, '. Num RESs: ', net.sgen.index.shape, '. RESs p: ',profile_res_p.shape)\n",
    "for i in range(num_episodes):\n",
    "\tt1 = time.time()\n",
    "\tif (i == num_episodes - 1):\n",
    "\t\tsave = True\n",
    "\telse:\n",
    "\t\tsave = False\n",
    "\t\n",
    "\tif(rl):\n",
    "\t\tfor_rl = {'agent': rlagent, 'save': save, 'train':train,\n",
    "\t\t\t'vm_pu_labels': np.squeeze(np.array(vm_pu_classification))} #np.squeeze(np.array(vm_pu_classification))\n",
    "\t\trun_timeseries(net,time_steps, for_rl)\n",
    "\t\tt2 = time.time()\n",
    "\t\tprint(f'Run {i+1}/{num_episodes}, time : {(t2-t1):.2f} s ({((t2-t1)/60):.1f} m)')\n",
    "\n",
    "\t\thistory = rlagent.history\n",
    "\t\tps = []\n",
    "\t\tqs = []\n",
    "\t\tvv1s = []\n",
    "\t\tvv2s = []\n",
    "\t\trs = []\n",
    "\t\tfor p,q,vv1,vv2,r in history:\n",
    "\t\t\tps.append(p)\n",
    "\t\t\tqs.append(q)\n",
    "\t\t\tvv1s.append(vv1)\n",
    "\t\t\tvv2s.append(vv2)\n",
    "\t\t\trs.append(r)\n",
    "\t\t# plt.scatter(range(len(rlagent.history)),rlagent.history,s=1)\n",
    "\t\t# losses = pd.DataFrame(rlagent.history)\n",
    "\t\t# losses.rolling(4*24).mean().plot()\n",
    "\t\ts = 1.5\n",
    "\t\tplt.scatter(range(len(ps)),np.array(ps),s=s, label='p')\n",
    "\t\tplt.scatter(range(len(qs)),np.array(qs),s=s, label='q')\n",
    "\t\tplt.scatter(range(len(vv1s)),np.array(vv1s),s=s, label='vv1')\n",
    "\t\tplt.scatter(range(len(vv2s)),np.array(vv2s),s=s, label='vv2')\n",
    "\t\tplt.scatter(range(len(rs)),rs,s=s, label='r')\n",
    "\t\tplt.legend()\n",
    "\t\t# plt.ylim([-500,500])\n",
    "\t\tplt.show()\n",
    "\telse:\n",
    "\t\trun_timeseries(net,time_steps)\n",
    "t_end = time.time()\n",
    "print(f'Total Simulation time : {(t_end-t_start):.2f} s ({((t_end-t_start)/60):.1f} m)')\n",
    "\n",
    "#Save time series (input)\n",
    "if(rl is False):\n",
    "\tpath = os.path.join(output_dir, \"loads_p.csv\")\n",
    "\t(cc_lp.data_source.df*cc_lp.scale_factor).to_csv(path)\n",
    "\tpath = os.path.join(output_dir, \"loads_q.csv\")\n",
    "\t(cc_lq.data_source.df*cc_lq.scale_factor).to_csv(path)\n",
    "\tpath = os.path.join(output_dir, \"RESs_p.csv\")\n",
    "\t(cc_sp.data_source.df*cc_sp.scale_factor).to_csv(path)\n",
    "\n",
    "\tpath = os.path.join(output_dir, \"noscale_loads_p.csv\")\n",
    "\t(cc_lp.data_source.df).to_csv(path)\n",
    "\tpath = os.path.join(output_dir, \"noscale_loads_q.csv\")\n",
    "\t(cc_lq.data_source.df).to_csv(path)\n",
    "\tpath = os.path.join(output_dir, \"noscale_RESs_p.csv\")\n",
    "\t(cc_sp.data_source.df).to_csv(path)\n",
    "\tt3 = time.time()\n",
    "\tprint(f'Saving files time: {(t3-t_end):.2f} s')\n",
    "\n",
    "#Total Simulation time : 521.57 s (8.7 m)\n",
    "#Total Simulation time : 1775.19 s (29.6 m)\n",
    "# print(rlagent.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = rlagent.history\n",
    "ps = []\n",
    "qs = []\n",
    "vv1s = []\n",
    "vv2s = []\n",
    "rs = []\n",
    "for p,q,vv1,vv2,r in history:\n",
    "\tps.append(p)\n",
    "\tqs.append(q)\n",
    "\tvv1s.append(vv1)\n",
    "\tvv2s.append(vv2)\n",
    "\trs.append(r)\n",
    "# plt.scatter(range(len(rlagent.history)),rlagent.history,s=1)\n",
    "# losses = pd.DataFrame(rlagent.history)\n",
    "# losses.rolling(4*24).mean().plot()\n",
    "s = 1.5\n",
    "plt.scatter(range(len(ps)),np.array(ps),s=s, label='p')\n",
    "plt.scatter(range(len(qs)),np.array(qs),s=s, label='q')\n",
    "plt.scatter(range(len(vv1s)),np.array(vv1s),s=s, label='vv1')\n",
    "plt.scatter(range(len(vv2s)),np.array(vv2s),s=s, label='vv2')\n",
    "\n",
    "# plt.scatter(range(len(ps)),np.array(ps)/alpha_p,s=s, label='p')\n",
    "# plt.scatter(range(len(qs)),np.array(qs)/alpha_q,s=s, label='q')\n",
    "# plt.scatter(range(len(vv1s)),np.array(vv1s)/beta,s=s, label='vv1')\n",
    "# plt.scatter(range(len(vv2s)),np.array(vv2s)/gamma,s=s, label='vv2')\n",
    "# plt.scatter(range(len(rs)),rs,s=s, label='r')\n",
    "plt.legend()\n",
    "# plt.ylim([-60,1])\n",
    "plt.show()\n",
    "\n",
    "losses = pd.DataFrame(rs)\n",
    "losses.rolling(4*24).mean().plot()\n",
    "# plt.ylim([-30,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = rlagent.history_test\n",
    "ps = []\n",
    "qs = []\n",
    "vv1s = []\n",
    "vv2s = []\n",
    "rs = []\n",
    "for p,q,vv1,vv2,r in history:\n",
    "\tps.append(p)\n",
    "\tqs.append(q)\n",
    "\tvv1s.append(vv1)\n",
    "\tvv2s.append(vv2)\n",
    "\trs.append(r)\n",
    "s = 1.5\n",
    "plt.scatter(range(len(vv2s)),np.array(vv2s),s=s, label='vv2')\n",
    "plt.scatter(range(len(ps)),np.array(ps),s=s, label='p')\n",
    "plt.scatter(range(len(qs)),np.array(qs),s=s, label='q')\n",
    "plt.scatter(range(len(vv1s)),np.array(vv1s),s=s, label='vv1')\n",
    "plt.scatter(range(len(rs)),np.array(rs),s=s, label='total r')\n",
    "plt.legend()\n",
    "\n",
    "losses = pd.DataFrame(rs)\n",
    "losses.rolling(4*12).mean().plot()\n",
    "# plt.ylim([-60,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = [i[0] for i in rlagent.q_values]\n",
    "q_values_tar = [i[1] for i in rlagent.q_values]\n",
    "plt.scatter(range(len(q_values)),np.array(q_values),s=s, label='q values')\n",
    "plt.scatter(range(len(q_values_tar)),np.array(q_values_tar),s=s, label='q values_tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = rlagent.noises\n",
    "plt.scatter(range(len(noises)),np.array(noises),s=s, label='noise evolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss = [ i[0] for i in rlagent.losses]\n",
    "actor_loss = [ i[1] for i in rlagent.losses]\n",
    "\n",
    "plt.scatter(range(len(critic_loss)), critic_loss, s=s, label='critic loss')\n",
    "plt.scatter(range(len(actor_loss)), actor_loss, s=s, label='actor loss')\n",
    "plt.ylim([-1,20])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = []\n",
    "mw_curt = []\n",
    "mw_tot = []\n",
    "q_tot = []\n",
    "q_tot_abs = []\n",
    "for p,v,t,q,qa in rlagent.curtailment:\n",
    "\tperc.append(p)\n",
    "\tmw_curt.append(v)\n",
    "\tmw_tot.append(t)\n",
    "\tq_tot.append(q)\n",
    "\tq_tot_abs.append(qa)\n",
    "perc = perc[-int(n_timesteps * (1-split)):]\n",
    "mw_curt = mw_curt[-int(n_timesteps * (1-split)):]\n",
    "mw_tot = mw_tot[-int(n_timesteps * (1-split)):]\n",
    "q_tot = q_tot[-int(n_timesteps * (1-split)):]\n",
    "q_tot_abs = q_tot_abs[-int(n_timesteps * (1-split)):]\n",
    "print(len(rlagent.curtailment))\n",
    "print(int(len(perc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(perc)),perc,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(mw_curt)),mw_curt,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(q_tot)),q_tot,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(q_tot_abs)),q_tot_abs,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_x = [0]*input_dim\n",
    "dummy_x = np.random.rand(input_dim) + 0.3*0#**3000\n",
    "# dummy_x = tf.concat([net.load.p_mw,net.load.q_mvar,net.sgen.p_mw],0)\n",
    "print(np.min(dummy_x))\n",
    "print(np.max(dummy_x))\n",
    "print(np.mean(dummy_x))\n",
    "print(np.std(dummy_x),'\\n')\n",
    "pred = rlagent.choose_action(dummy_x)\n",
    "print(np.min(pred))\n",
    "print(np.max(pred))\n",
    "print(np.mean(pred))\n",
    "print(np.std(pred), '\\n')\n",
    "plt.scatter(range(len(pred)),pred, s=3)\n",
    "\n",
    "# print(pred)\n",
    "indx = np.where(pred == 1)[0] #25, 37\n",
    "print(indx)\n",
    "# print(net.sgen.index)\n",
    "# print(net.sgen.index[indx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indx:\n",
    "\tfig = cc_sp.data_source.df[net.sgen.index[i]].plot()\n",
    "\tfig.plot()\n",
    "\tplt.plot()\n",
    "\tl = 0.0000001\n",
    "\tplt.ylim([-l,l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total energy {np.sum(mw_tot):.0f} MW, total energy curtailment: {np.sum(mw_curt):.1f} MW in {len(mw_curt)} timesteps(~{int(len(mw_curt)/(4*24*365)*12)} months), ratio: {(np.sum(mw_curt)/np.sum(mw_tot)*100):.2f}% \\naverage for 15mins: {(np.mean(mw_curt)):.2f}+/-{np.std(mw_curt):.2f} MW, average for 15mins per RES device: {(np.mean(mw_curt)/n_res*1000):.2f} kW')\n",
    "print(f'\\n{np.sum(q_tot):.5f} {np.max(q_tot):.5f} {np.min(q_tot):.5f} {np.mean(q_tot):.5f} {np.std(q_tot):.5f}')\n",
    "print(f'\\n{np.sum(q_tot_abs):.5f} {np.max(q_tot_abs):.5f} {np.min(q_tot_abs):.5f} {np.mean(q_tot_abs):.5f} {np.std(q_tot_abs):.5f}')\n",
    "# print(/len(perc))\n",
    "# print(np.sum(val)/len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curt_needed = []\n",
    "curt_not_needed = []\n",
    "a = 0\n",
    "for i in range(len(vm_pu_classification_a)):\n",
    "\tif(i<len(vm_pu_classification_a)-1):\n",
    "\t\tproblem_before = vm_pu_classification_b.loc[i+1][0]\n",
    "\t\tproblem_after = vm_pu_classification_a.loc[i+1][0]\n",
    "\t\tif(problem_before==1 and problem_after==0):\n",
    "\t\t\tcurt_needed.append([perc[i],mw_curt[i]])\n",
    "\t\telse:\n",
    "\t\t\tcurt_not_needed.append([perc[i],mw_curt[i]])\n",
    "\n",
    "\t\t\n",
    "print(f'Curtailment needed: {(np.sum([i[1] for i in curt_needed])):.2f}, Curtailment not needed: {(np.sum([i[1] for i in curt_not_needed])):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1000\n",
    "fig = px.line(cc_lp.data_source.df*cc_lp.scale_factor,x=time_steps, y=profile_load_p.columns, width=width, height=400)\n",
    "fig.show()\n",
    "fig = px.line(cc_lq.data_source.df*cc_lq.scale_factor,x=time_steps, y=profile_load_q.columns, width=width, height=400)\n",
    "fig.show()\n",
    "fig = px.line(cc_sp.data_source.df*cc_sp.scale_factor,x=time_steps, y=profile_res_p.columns, width=width, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df,title='',x_axis='',y_axis='',file_name='', ticks=''):\n",
    "\tfig = px.line(df,x=time_steps, y=df.columns, width=1200, height=600)\n",
    "\tfig.update_layout(title=title,\n",
    "\t\t\t\t\txaxis_title=x_axis,\n",
    "\t\t\t\t\tyaxis_title=y_axis)\n",
    "\n",
    "\tx = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\tfig.update_xaxes(tickangle=-45,\n",
    "\t\t\t\t\ttickmode = 'array',\n",
    "\t\t\t\t\ttickvals = x,\n",
    "\t\t\t\t\tticktext= list(ticks))\n",
    "\tif(file_name):\n",
    "\t\tfig.write_html(file_name)\n",
    "\t\tprint(f'Saved \\'{title}\\' in \\'{file_name}\\'')\n",
    "\t# fig.show()\n",
    "\n",
    "# output_dir = os.path.join(input_dir,\"Results\", str(case))\n",
    "save = False\n",
    "x_axis = ''\n",
    "\n",
    "# voltage results\n",
    "vm_pu_file = f'{output_dir}/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu = vm_pu.drop(columns='58' ) #External grid\n",
    "if(save):\n",
    "\tplot_df(vm_pu,'buses voltage magnitude',x_axis, 'bus vm [pu]', os.path.join(output_dir, \"Plots\", \"bus vm.html\"), ticks=ticks)\n",
    "\n",
    "# # line loading resulcsvts\n",
    "ll_file = os.path.join(output_dir, \"res_line\", \"loading_percent.csv\")\n",
    "line_loading = pd.read_csv(ll_file, index_col=0, sep=';')\n",
    "if(save):\n",
    "\tplot_df(line_loading,'line_loading',x_axis, 'line_loading [%]', os.path.join(output_dir, \"Plots\", \"line load.html\"), ticks=ticks)\n",
    "\n",
    "# # load results\n",
    "load_file = os.path.join(output_dir, \"res_load\", \"p_mw.csv\")\n",
    "load = pd.read_csv(load_file, index_col=0, sep=';')\n",
    "if(save):\n",
    "\tplot_df(load,'load active power',x_axis, 'p [MW]', os.path.join(output_dir, \"Plots\", \"load.html\"), ticks=ticks)\n",
    "load_file = os.path.join(output_dir, \"res_load\", \"q_mvar.csv\")\n",
    "load_q = pd.read_csv(load_file, index_col=0, sep=';')\n",
    "\n",
    "res_prepf = pd.read_csv(os.path.join(output_dir, \"RESs_p.csv\"))\n",
    "res_prepf = res_prepf.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_pu_file = f'./TimeSeries/1-MVLV-rural-all-0-sw/Results/reference/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu_before = vm_pu.drop(columns='58' ) #External grid\n",
    "print(len(vm_pu_before))\n",
    "vm_pu_before = vm_pu_before[-int(n_timesteps * (1-split)):]\n",
    "print(len(vm_pu_before))\n",
    "\n",
    "vm_pu_file = f'./TimeSeries/1-MVLV-rural-all-0-sw/Results/rl/Testing/res_bus/vm_pu.csv'\n",
    "vm_pu = pd.read_csv(vm_pu_file, index_col=0, sep=';')\n",
    "vm_pu_after = vm_pu.drop(columns='58' ) #External grid\n",
    "print(len(vm_pu_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(22, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "(res_prepf).sum(axis=1).plot(label='load p',ylabel='mw')\n",
    "(load).sum(axis=1).plot(label='sgen')\n",
    "\n",
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))\n",
    "\n",
    "plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend()\n",
    "# plt.xlabel('time')\n",
    "plt.title('Total load and generation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_pu.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# vm_pu = vm_pu_before\n",
    "vm_pu = vm_pu_after\n",
    "df['max'] = vm_pu.max(axis=1)\n",
    "df['min'] = vm_pu.min(axis=1)\n",
    "ch = []\n",
    "cl = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\tif(row['max']>1.05):\n",
    "\t\tch.append('r')\n",
    "\telse:\n",
    "\t\tch.append('b')\n",
    "\tif(row['min']<0.95):\n",
    "\t\tcl.append('r')\n",
    "\telse:\n",
    "\t\tcl.append('b')\n",
    "\t\t\n",
    "df['over'] = ch\n",
    "df['under'] = cl\n",
    "# # d = {True: 'Voltage problem', False: 'Normal condition'}\n",
    "# # df = df.replace(d)\n",
    "# fig = px.scatter(df,x=df.index,y='max',color='over', width=1000, height=400)\n",
    "# # fig.update_yaxes(type='category')\n",
    "# fig.update_traces(marker_size=5)\n",
    "# fig.update_layout(\n",
    "# \ttitle=\"\",\n",
    "# \txaxis_title=\"Time steps\",\n",
    "# \tyaxis_title=\"max bus vm [pu]\",\n",
    "# \tlegend_title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# ticks = range(1,91)\n",
    "\n",
    "x = np.linspace(0,n_timesteps*(len(ticks)-1)/len(ticks), num=len(ticks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "s = 10\n",
    "ma = plt.scatter(x=df.index,y=df['max'],c=df['over'],s=s, marker='^')\n",
    "mi = plt.scatter(x=df.index,y=df['min'],c=df['under'],s=s, marker='v')\n",
    "\n",
    "plt.scatter(x=[],y=[],c=['r'], marker='^', s=s, label='Maxs overvoltages')\n",
    "plt.scatter(x=[],y=[],c=['b'], marker='^', s=s, label='Maxs normal')\n",
    "plt.scatter(x=[],y=[],c=['r'], marker='v', s=s, label='Mins undervoltages')\n",
    "plt.scatter(x=[],y=[],c=['b'], marker='v', s=s, label='Mins normal')\n",
    "\n",
    "# plt.axvline(x=n_timesteps * 0.7)\n",
    "# plt.axvline(x=n_timesteps * 0.9)\n",
    "# plt.axhline(y=1.05)\n",
    "# plt.axhline(y=0.95)\n",
    "# plt.axhline(y=df['max'].mean())\n",
    "plt.axhline(y=1.05)\n",
    "plt.axhline(y=0.95)\n",
    "print(df['max'].mean(), df['max'].max(), df['max'].min())\n",
    "# print(df['min'].mean(), df['min'].max(), df['min'].min())\n",
    "\n",
    "# plt.xticks(x, ticks, rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot network and loadings\n",
    "\n",
    "bus = 58\n",
    "fig = simple_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "fig.add_trace(px.scatter(x=[net.bus_geodata.loc[bus, 'x']], y=[net.bus_geodata.loc[bus, 'y']],color=['r'],size=[10]).data[0])\n",
    "# _ = vlevel_plotly(net, bus_size=5, ext_grid_size=10)\n",
    "# fig = pf_res_plotly(net, bus_size=8)\n",
    "# fig.write_html(f\"images/MVOberrhein/Half2.html\")\n",
    "# fig.write_image(f\"images/MVOberrhein/Half2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vm_pu[58]) #constant voltage?\n",
    "print(net.bus.loc[95])\n",
    "print(net.load[net.load['bus']==58]) #no load in bus 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification sources:\n",
    "# - https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "#Naive version\n",
    "def classification_dataset(df):\n",
    "\tlabels = []\n",
    "\tln = []\n",
    "\ta = 0\n",
    "\tn=0\n",
    "\tacceptance_range = 0.1\n",
    "\tfor i in df.to_numpy():\n",
    "\t\tif(np.any(i>1+acceptance_range/2)):# or np.any(i<1-acceptance_range/2)\n",
    "\t\t\tlabels.append(1)\n",
    "\t\t\ta+=1\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\tif(np.any(i<1-acceptance_range/2)):#\n",
    "\t\t\tln.append(1)\n",
    "\t\t\tn+=1\n",
    "\t\telse:\n",
    "\t\t\tln.append(0)\n",
    "\tprint(f'Number of critical situations: {a}, over {len(df)} time steps, ratio: {(a/len(df)*100):.1f}%')\n",
    "\tprint(f'Number of critical situations(underv): {n}, over {len(ln)} time steps, ratio: {(n/len(ln)*100):.1f}%')\n",
    "\treturn pd.DataFrame(labels,columns=['Label'])\n",
    "\n",
    "#Less naive\n",
    "# def f(window):\n",
    "# \treturn max(window)\n",
    "# vm_pu['132'].rolling(3).apply(f)\n",
    "\n",
    "vm_pu_classification = classification_dataset(vm_pu)\n",
    "print()\n",
    "vm_pu_classification_b = classification_dataset(vm_pu_before)\n",
    "vm_pu_classification_a = classification_dataset(vm_pu_after)\n",
    "\n",
    "# Number of critical situations: 957, over 21024 time steps, ratio: 4.6%\n",
    "# Number of critical situations(underv): 138, over 21024 time steps, ratio: 0.7%\n",
    "# Number of critical situations: 828, over 21024 time steps, ratio: 3.9%\n",
    "# Number of critical situations(underv): 143, over 21024 time steps, ratio: 0.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([vm_pu], axis=1)\n",
    "# x = pd.concat([res_prepf], axis=1)\n",
    "y = vm_pu_classification\n",
    "deep_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlagent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_load_p = load.sum(axis=1)\n",
    "sum_load_q = load_q.sum(axis=1)\n",
    "sum_res_p = res_prepf.sum(axis=1)\n",
    "\n",
    "x = pd.concat([sum_load_p,sum_load_q,sum_res_p], axis=1)\n",
    "y = vm_pu_classification\n",
    "deep_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r,n,d = rlagent.memory.sample_experiences(2)\n",
    "print(s.shape)\n",
    "print(a.shape)\n",
    "print(r.shape)\n",
    "print(n.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple transformation of the series into a new time series, which we use to remove the series dependence on time and stabilize the mean of the time series, so trend and seasonality are reduced during this transformation.\n",
    "x = x.diff(axis = 0, periods = 1)\n",
    "#Remove NAN values\n",
    "x = x.drop(index=0,axis=1)\n",
    "y = y.drop(index=0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import standardize_mapping\n",
    "# x.columns = [f'l_{name}' for name in x.columns]\n",
    "# y.columns = [f'bvm_{name}' for name in y.columns]\n",
    "df = pd.concat([x], axis=1)\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "\n",
    "if(deep_models):\n",
    "\t#Maybe 0.65, 0.85 is better\n",
    "\t# train_portion = 0.8\n",
    "\t# val_portion = 0.9\n",
    "\ttrain_portion = 0.7\n",
    "\tval_portion = 0.9\n",
    "\n",
    "\ttrain_df = df[0:int(n*train_portion)]\n",
    "\tval_df = df[int(n*train_portion):int(n*val_portion)]\n",
    "\ttest_df = df[int(n*val_portion):]\n",
    "\n",
    "\ttrain_df = pd.concat( [(train_df), y[0:int(n*train_portion)]], axis=1)\n",
    "\tval_df = pd.concat( [(val_df), y[int(n*train_portion):int(n*val_portion)]], axis=1)\n",
    "\ttest_df = pd.concat( [(test_df), y[int(n*val_portion):]], axis=1)\n",
    "\tprint(train_df.shape,val_df.shape,test_df.shape)\n",
    "else:\n",
    "\ttrain_portion = 0.8\n",
    "\ttrain_df = df[0:int(n*train_portion)]\n",
    "\ttest_df = df[int(n*train_portion):]\n",
    "\n",
    "\ttrain_mean = train_df.mean()\n",
    "\ttrain_std = train_df.std()\n",
    "\n",
    "\ttrain_df = pd.concat( [(train_df - train_mean) / train_std,y[0:int(n*train_portion)]], axis=1)\n",
    "\ttest_df = pd.concat( [(test_df - train_mean) / train_std,y[int(n*train_portion):]], axis=1)\n",
    "\tprint(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(deep_models):\n",
    "\ta = y[0:int(n*train_portion)]\n",
    "\tb = y[int(n*train_portion):int(n*val_portion)]\n",
    "\tc = y[int(n*val_portion):]\n",
    "\t# print(f'Total # of critical instants: {list(a.sum()+b.sum()+c.sum())[0]}, ratio: {(list((a.sum()+b.sum()+c.sum())/(a.count()+b.count()+c.count())*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Train set: {list(a.sum())[0]}, ratio: {(list(a.sum()/a.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Val set: {list(b.sum())[0]}, ratio: {(list(b.sum()/b.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Test set: {list(c.sum())[0]}, ratio: {(list(c.sum()/c.count()*100)[0]):.1f}%')\n",
    "else:\n",
    "\ta = y[0:int(n*train_portion)]\n",
    "\tc = y[int(n*train_portion):]\n",
    "\t# print(f'Total # of critical instants: {list(a.sum()+b.sum()+c.sum())[0]}, ratio: {(list((a.sum()+b.sum()+c.sum())/(a.count()+b.count()+c.count())*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Train set: {list(a.sum())[0]}, ratio: {(list(a.sum()/a.count()*100)[0]):.1f}%')\n",
    "\tprint(f'\\State Number of critical instants in Test set: {list(c.sum())[0]}, ratio: {(list(c.sum()/c.count()*100)[0]):.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(a)\n",
    "pos = a.sum()\n",
    "neg = total - pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2)\n",
    "weight_for_1 = (1 / pos) * (total / 2) / 8\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(f'Weight for class 0: {weight_for_0:.2f}')\n",
    "print(f'Weight for class 1: {weight_for_1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "\tdef __init__(self, input_width, label_width, shift,\n",
    "\t\t\t\ttrain_df=train_df, val_df=val_df, test_df=test_df,\n",
    "\t\t\t\tall_columns =None, training_columns=None, label_columns=None):\n",
    "\t\t# Store the raw data.\n",
    "\t\tself.train_df = train_df\n",
    "\t\tself.val_df = val_df\n",
    "\t\tself.test_df = test_df\n",
    "\n",
    "\t\tself.all_columns = all_columns\n",
    "\t\tself.column_indices = {name: i for i, name in enumerate(self.all_columns)}\n",
    "\t\tprint('All columns: ', self.column_indices)\n",
    "\n",
    "\t\tself.training_columns = training_columns\n",
    "\t\t# print('Train columns: ', self.column_indices_t)\n",
    "\n",
    "\t\tself.label_columns = label_columns\n",
    "\t\t# print('Label columns: ', self.column_indices_l)\n",
    "\n",
    "\t\t# Work out the window parameters.\n",
    "\t\tself.input_width = input_width\n",
    "\t\tself.label_width = label_width\n",
    "\t\tself.shift = shift\n",
    "\n",
    "\t\tself.total_window_size = input_width + shift\n",
    "\n",
    "\t\tself.input_slice = slice(0, input_width)\n",
    "\t\tself.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "\t\tself.label_start = self.total_window_size - self.label_width\n",
    "\t\tself.labels_slice = slice(self.label_start, None)\n",
    "\t\tself.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn '\\n'.join([\n",
    "\t\t\tf'Total window size: {self.total_window_size}',\n",
    "\t\t\tf'Input indices: {self.input_indices}',\n",
    "\t\t\tf'Label indices: {self.label_indices}',\n",
    "\t\t\tf'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "\tdef split_window(self, features):\n",
    "\t\tinputs = features[:, self.input_slice, :]\n",
    "\t\tindices = [self.column_indices[c] for c in self.training_columns]\n",
    "\t\t# inputs = tf.stack( [inputs[:, :, self.all_columns.index(name)] for name in self.training_columns], axis=-1)\n",
    "\t\tinputs = tf.stack( [inputs[:, :, i] for i in indices], axis=-1)\n",
    "\t\tindices = [self.column_indices[c] for c in self.training_columns]\n",
    "\n",
    "\t\t# labels = features[:, self.labels_slice, :]\n",
    "\t\tlabels = features[:, self.labels_slice, :]\n",
    "\t\tindices = [self.column_indices[c] for c in self.label_columns]\n",
    "\n",
    "\t\tlabels = tf.stack( [labels[:, :, i] for i in indices], axis=-1)\n",
    "\t\t# labels = tf.stack( l, axis=-1)\n",
    "\n",
    "\n",
    "\t\t# Slicing doesn't preserve static shape information, so set the shapes\n",
    "\t\t# manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "\t\tinputs.set_shape([None, self.input_width, None])\n",
    "\t\t# labels.set_shape([None, self.label_width, None])\n",
    "\t\tlabels = tf.reshape(labels, [-1,self.label_width])\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef plot(self, model=None):\n",
    "\t\tpredictions = []\n",
    "\t\tlabels = []\n",
    "\t\ta = 0\n",
    "\t\tfor i,l in self.test:\n",
    "\t\t\tif model is not None:\n",
    "\t\t\t\tpredictions.append(list(tf.reshape(model(i),[-1]).numpy()))\n",
    "\t\t\ta+=1\n",
    "\t\t\tlabels.append(list(tf.reshape(l,[-1]).numpy()))\n",
    "\t\tpredictions = [item for sublist in predictions for item in sublist]\n",
    "\t\tpredictions = np.round(predictions)+0.005 # for visualization purposes\n",
    "\t\tlabels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "\n",
    "\t\tx = range(len(predictions))\n",
    "\t\tplt.figure(figsize=(16,6))\n",
    "\t\ts = 1\n",
    "\t\tplt.scatter(x, predictions,s=s)\n",
    "\t\tplt.scatter(x, labels,s=s)\n",
    "\t\tplt.show()\n",
    "\t\treturn predictions,labels\n",
    "\n",
    "\tdef make_dataset(self, data):\n",
    "\t\tdata = np.array(data, dtype=np.float32)\n",
    "\t\tds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "\t\t\tdata=data,\n",
    "\t\t\ttargets=None,\n",
    "\t\t\tsequence_length=self.total_window_size,\n",
    "\t\t\tsequence_stride=1,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tbatch_size=1024)\n",
    "\t\t\t\n",
    "\t\tds = ds.map(self.split_window)\n",
    "\n",
    "\t\treturn ds\n",
    "\n",
    "\t@property\n",
    "\tdef train(self):\n",
    "\t\treturn self.make_dataset(self.train_df)\n",
    "\n",
    "\t@property\n",
    "\tdef val(self):\n",
    "\t\treturn self.make_dataset(self.val_df)\n",
    "\n",
    "\t@property\n",
    "\tdef test(self):\n",
    "\t\treturn self.make_dataset(self.test_df)\n",
    "\n",
    "\t@property\n",
    "\tdef example(self):\n",
    "\t\t\"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "\t\tresult = getattr(self, '_example', None)\n",
    "\t\tif result is None:\n",
    "\t\t\t# No example batch was found, so get one from the `.train` dataset\n",
    "\t\t\tresult = next(iter(self.test))\n",
    "\t\t# And cache it for next time\n",
    "\t\tself._example = result\n",
    "\t\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "\tdef __init__(self, input_width, label_width, mean, std, batch_size):\n",
    "\t\tself.input_width = input_width\n",
    "\t\tself.label_width = label_width\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, mean.shape[0] ])\n",
    "\t\tself.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, mean.shape[0] ])\n",
    "\n",
    "\tdef split_window(self, features):\n",
    "\t\tinputs = features[:, :-self.label_width, :-1]\n",
    "\n",
    "\t\tlabels = features[:, -self.label_width:, -1]\n",
    "\t\t\n",
    "\t\tinputs.set_shape([None, self.input_width, inputs.shape[-1]])\n",
    "\t\tlabels.set_shape([None,self.label_width])\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef normalize(self, features):\n",
    "\t\tfeatures = (features - self.mean) / (self.std + 1.e-6)\n",
    "\t\treturn features\n",
    "\n",
    "\tdef preprocess(self, features):\n",
    "\t\tinputs, labels = self.split_window(features)\n",
    "\t\tinputs = self.normalize(inputs)\n",
    "\n",
    "\t\treturn inputs, labels\n",
    "\n",
    "\tdef make_dataset(self, data, train):\n",
    "\t\t# print('a',data.shape)\n",
    "\t\t#The targets is None since the labels are already inside the data\n",
    "\t\tds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "\t\t\t\t\t\tdata=data,\n",
    "\t\t\t\t\t\ttargets=None,\n",
    "\t\t\t\t\t\tsequence_length=self.input_width+self.label_width,\n",
    "\t\t\t\t\t\tsequence_stride=1,\n",
    "\t\t\t\t\t\tbatch_size=self.batch_size)\n",
    "\t\t# print(ds, ds.)\n",
    "\t\tds = ds.map(self.preprocess)\n",
    "\t\tds = ds.cache()\n",
    "\t\tif train is True:\n",
    "\t\t\tds = ds.shuffle(1024, reshuffle_each_iteration=True)\n",
    "\n",
    "\t\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 4*4\n",
    "output_window = 1\n",
    "label_columns = y.columns\n",
    "mean = train_df.mean(axis=0)[:-1] #remove labels from the standardization process\n",
    "std = train_df.std(axis=0)[:-1] #remove labels from the standardization process\n",
    "if(deep_models):\n",
    "  batch_size = 512\n",
    "else:\n",
    "  batch_size=1\n",
    "generator = WindowGenerator(input_width=input_window, label_width=output_window, mean=mean, std=std, batch_size=batch_size)\n",
    "train_ds = generator.make_dataset(train_df, True)\n",
    "if(deep_models):\n",
    "  val_ds = generator.make_dataset(val_df, False)\n",
    "test_ds = generator.make_dataset(test_df, False)\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx,yy in train_ds.take(2):\n",
    "\tprint(xx.shape) #(32,6,2)\n",
    "\tprint(yy.shape, np.sum(yy),'\\n') #(32,6,1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_df.columns\n",
    "training_columns = x.columns\n",
    "label_columns = y.columns\n",
    "input_window = 4*4\n",
    "output_window = 1\n",
    "shift = output_window\n",
    "dwg = WindowGenerator(input_window,output_window,shift,all_columns=all_columns,training_columns=training_columns,label_columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in dwg.train.take(1):\n",
    "  print(f'Inputs shape (batch size, time steps, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch size, time steps, [features]): {example_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.Sequential([\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(units=32*4, activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=32*2, activation='relu'),\n",
    "\ttf.keras.layers.Dropout(0.5),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])\n",
    "cnn = tf.keras.Sequential([\n",
    "\ttf.keras.layers.Conv1D(filters=int(64*3),kernel_size=(4,),activation='relu'),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(int(64*2), activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])\n",
    "\n",
    "rnn = tf.keras.Sequential([\n",
    "\ttf.keras.layers.LSTM(units=int(64*3)),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(int(64*2), activation='relu'),\n",
    "\ttf.keras.layers.Dense(units=output_window*len(label_columns), activation='sigmoid'),\n",
    "\t# tf.keras.layers.Reshape([output_window, len(label_columns)])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds is considered as 1 batch size (a,b,c) a = 1 (batch size)\n",
    "def convert_dataset_to_numpy(ds):\n",
    "\tlx = []\n",
    "\tly = []\n",
    "\tfor xx,yy in ds:\n",
    "\t\tshapes = xx.shape\n",
    "\t\txi = xx.numpy().reshape([ (shapes[1]*shapes[2]) ])\n",
    "\t\tyi = yy.numpy()\n",
    "\t\t# rand_forest.fit(yi,xi)\n",
    "\t\tlx.append(xi)\n",
    "\t\tly.append(yi)\n",
    "\tlx = np.array(lx)\n",
    "\tly = np.squeeze(np.array(ly))\n",
    "\treturn lx,ly\n",
    "\n",
    "train_timeseries = convert_dataset_to_numpy(train_ds)\n",
    "test_timeseries = convert_dataset_to_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "def get_score(classifier,train,test):\n",
    "\tclassifier.fit(train[0], train[1])\n",
    "\ty_pred = classifier.predict(test[0])\n",
    "\taccuracy = accuracy_score(test[1], y_pred)\n",
    "\tprecision = precision_score(test[1], y_pred, average='macro')\n",
    "\trecall = recall_score(test[1], y_pred, average='macro')\n",
    "\tf1_score_ = f1_score(test[1], y_pred, average='macro')\n",
    "\tprint(f\"#Accuracy: {accuracy:.5f}\")\n",
    "\tprint(f\"#Precision: {precision:.5f}\")\n",
    "\tprint(f\"#Recall: {recall:.5f}\")\n",
    "\tprint(f\"#f1 Score: {f1_score_:.5f}\")\n",
    "\treturn precision,recall,f1_score_,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=50, random_state=0)#, class_weight=class_weight\n",
    "# clf.fit(train_timeseries[0],train_timeseries[1])\n",
    "_ = get_score(clf, train_timeseries, test_timeseries)\n",
    "#Best\n",
    "# clf = sklearn.ensemble.RandomForestClassifier(n_estimators=500, max_depth=50, random_state=0)\n",
    "#Accuracy: 0.95097\n",
    "#Precision: 0.91891\n",
    "#Recall: 0.85007\n",
    "#f1 Score: 0.88019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "monitor = 'val_recall'\n",
    "def compile_and_fit(model, window, patience=6,class_weight=[], checkpoint_filepath=''):\n",
    "\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=patience,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmode='max',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tverbose=1)\n",
    "\tckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\tcheckpoint_filepath,\n",
    "\t\tmonitor='val_recall',\n",
    "\t\tverbose=0,\n",
    "\t\tsave_best_only=True,\n",
    "\t\tsave_weights_only=True,\n",
    "\t\tmode='max',\n",
    "\t\tsave_freq='epoch',\n",
    "\t\toptions=None,\n",
    "\t\tinitial_value_threshold=None,\n",
    "\t\t)\n",
    "\tprint(f\"checkpoint in {checkpoint_filepath}\")\n",
    "\n",
    "\tmodel.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "\t\t\t\toptimizer=tf.optimizers.Adam(),\n",
    "\t\t\t\tmetrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Precision(name='precision'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Recall(name='recall')]\n",
    "\t\t\t\t)\n",
    "\n",
    "\thistory = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "\t\t\t\t\t\tvalidation_data=window.val,\n",
    "\t\t\t\t\t\tcallbacks=[ckpoint],\n",
    "\t\t\t\t\t\tclass_weight=class_weight)#early_stopping\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "monitor = 'val_recall'\n",
    "def compile_and_fit(model, train, val, patience=6,class_weight=[], checkpoint_filepath=''):\n",
    "\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=patience,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmode='max',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tverbose=1)\n",
    "\tckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\tcheckpoint_filepath,\n",
    "\t\tmonitor='val_recall',\n",
    "\t\tverbose=1,\n",
    "\t\tsave_best_only=True,\n",
    "\t\tsave_weights_only=True,\n",
    "\t\tmode='max',\n",
    "\t\tsave_freq='epoch',\n",
    "\t\toptions=None,\n",
    "\t\tinitial_value_threshold=None,\n",
    "\t\t)\n",
    "\tprint(f\"checkpoint in {checkpoint_filepath}\")\n",
    "\n",
    "\tmodel.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "\t\t\t\toptimizer=tf.optimizers.Adam(),\n",
    "\t\t\t\tmetrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Precision(name='precision'),\n",
    "\t\t\t\t\t\t\ttf.keras.metrics.Recall(name='recall')]\n",
    "\t\t\t\t)\n",
    "\n",
    "\thistory = model.fit(train, epochs=MAX_EPOCHS,\n",
    "\t\t\t\t\t\tvalidation_data=val,\n",
    "\t\t\t\t\t\tcallbacks=[ckpoint],\n",
    "\t\t\t\t\t\tclass_weight=class_weight)#early_stopping\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense\n",
    "checkpoint_filepath = output_dir+'/Model_ckp/dense'+'/checkpoint/'\n",
    "history = compile_and_fit(model, dwg, class_weight=[], checkpoint_filepath=checkpoint_filepath)\n",
    "# history = compile_and_fit(model, dwg, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense\n",
    "checkpoint_filepath = output_dir+'/Model_ckp/dense'+'/checkpoint/' \n",
    "history = compile_and_fit(model, train_ds, val_ds, class_weight=[], checkpoint_filepath=checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = np.arange(0,MAX_EPOCHS)\n",
    "def plot_history(x,history,column,label_name):\n",
    "\tplt.plot(\n",
    "\t\tx,\n",
    "\t\thistory.history[column], label=label_name,\n",
    "\t\tmarker='o',\n",
    "\t\tmarkersize=3\n",
    ")\n",
    "plot_history(x_a,history,'accuracy','Accuracy')\n",
    "plot_history(x_a,history,'precision','Precision')\n",
    "plot_history(x_a,history,'recall','Recall')\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = np.arange(0,MAX_EPOCHS)\n",
    "# x = np.arange(0,42)\n",
    "plot_history(x_a,history,'val_accuracy','Val Accuracy')\n",
    "plot_history(x_a,history,'val_precision','Val Precision')\n",
    "plot_history(x_a,history,'val_recall','Val Recall')\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = dwg.plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(dwg.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(dwg.test)\n",
    "# print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx,yy in dwg.test:\n",
    "\tprint(xx.shape,len(yy),np.sum(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.evaluate(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40e0f7cfb8db8545ff14d9c3144ceb50924ab3074a39872a41bc717ddda6db7d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('gyn_anm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
